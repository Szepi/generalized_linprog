\begin{abstract}
Approximate linear programming (ALP) and its variants have been widely applied to Markov Decision Processes (MDPs) with a large number of states. A serious limitation of ALP is that it has an intractable number of constraints, as a result of which constraint approximations are of interest. In this paper, we define a generalized reduced linear program (GRLP) that has a tractable number of constraints, which are obtained as positive linear combinations of the original constraints of the ALP. The main contribution of this paper is a novel theoretical framework developed to obtain error bounds for any given GRLP. By providing a detailed error analysis for the GRLP, we justify usage of a linear architecture for approximating the dual variables. Unlike prior results on constraint sampling, our analysis is deterministic and is based on a novel contraction operator.
\end{abstract}
\begin{keywords}{
Approximate Dynamic Programming (ADP), Markov Decision Processes (MDPs), Approximate Linear Programming (ALP), Generalized Reduced Linear Program (GRLP), Constraint Sampling, Reinforcement Learning.}
\end{keywords}
