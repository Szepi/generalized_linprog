%!TEX root =  autocontgrlp.tex
\section{Introduction}
Markov decision processes (MDPs) are a powerful mathematical framework to study optimal sequential decision making problems arising in science and engineering. Typically, the configuration of the underlying environment is known as the state and in decisions are chosen from a set of actions. Given an MDP, the objective is to compute a useful \emph{policy} $u$ (we defer a more formal definition to later) which is specifies the action selection mechanism, i.e., maps each and every state of the system to a corresponding action. The effectiveness of a policy $u$ is indicated by a corresponding value function $J_u$, a vector whose dimension is the number of states. The so-called dynamic programming methods find an optimal (best) policy $u^*$ by computing the optimal \emph{value-function} ($J^*$). MDPs with small number of states can be solved easily by conventional dynamic programming techniques, such as value-, or policy-iteration, or linear programming (LP) \cite{BertB}.\par

The paradigm of approximate dynamic programming (ADP) deals with the problem of using dynamic programming with MDPs with large state spaces. In practice, ADP methods handle the issue of large number of states by computing an approximate value function $\tilde{J}$ instead of $J^*$.
Linear function approximation (LFA), i.e., letting $\tilde{J}=\Phi r^*$ where $\Phi$ is a so-called feature matrix and $r^*$ is a weight vector to be computed, is the most widely used method of approximation. Here, dimensionality reduction is achieved by choosing $\Phi$ to have fewer columns in comparison to the number of states, holding the promise of being able to work with MDPs regardless of the number of states.
\todoch{Mention objectives}
Given a fixed LFA (i.e., a fixed $\Phi$), ADP methods vary in the manner in which they compute $\tilde{J}=\Phi r^*$. The so called projected Bellman equation (PBE) based methods compute $r^*$ by solving a fixed point equation. However, a downside to the PBE based methods is that they exhibit divergent behavior leading to a oscillatory policy behaviour \cite{BertB}. The \emph{approximate linear program} (ALP) \cite{ALP,CS,SALP,ALP-Bor} and its variants introduce linear function approximation in the linear programming formulation. The vanilla ALP method computes an approximate value function $\tilde{J}$ and an approximate policy $u$. A salient feature that the ALP methods enjoy over the PBE based methods is that they do not suffer from the issue of policy oscillation. In addition, the ALP method has good performance guarantees i.e., the error terms $\parallel J^*-\tj\parallel$ (error in approximating value function) and $\parallel J^*-J_{\tilde{u}}\parallel$ (loss in perfromance i.e., the sub-optimality of $\tilde{u}$ as opposed to $u^*$).\par
\todoch{Mention all the works including Malek's and the open problem we wish to address}
\todoch{Add the table with tick and cross and the cartoon}
A critical shortcoming of the vanilla ALP is that the number of constraints are of the order of the size of the state space, making this vanilla version intractable. Typically, a way out is to choose a subset of constraints at random and drop the rest, thereby formulating a \emph{reduced linear program} (RLP). The performance analysis of the RLP under idealized assumptions (of the knowledge of the optimal policy) can be found in \cite{CS} and the RLP has also been shown to perform well in experiments \cite{ALP,CS,CST}. An alternative approach to handle the issue of large number of constraints is to employ function approximation in the dual variables of the ALP \cite{ALP-Bor,dolgov}, an approach that was also found useful in experiments. However, to this date, there exist no theoretical guarantees bounding the loss in performance resulting from such an approximation. The dual ALP formulation has been studied in \cite{abbasi}, however without addressing the limitation posed by the large number of constraints.\par
\todoch{Mention Motivation and Challanges}
\todoch{Use of Lyanpunov analysis}
In this paper, we focus on constraint approximation, an aspect of the ALP which has not been well understood in the literature. In particular, we generalize the RLP to define a generalized reduced linear program (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints.
The salient aspects of our contribution are listed below:
\begin{enumerate}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$ by bounding the prediction error $||J^*-\hj||$ (\Cref{cmt2mn}). 
		\item We also bound the performance loss due to using the policy $\hu$ that is one-step greedy with respect to $\hj$ (Theorem~\ref{polthe}).
		\item Our analysis is based on two novel $\max$-norm contraction operators and our results hold \emph{deterministically}, as opposed to the results on RLP \cite{SALP,CS}, where the guarantees have a probabilistic nature.
\item Our analysis also makes use of arguments based on \emph{Lyanpunov} function, an approach much similar to prior works in ALP literature \cite{ALP,SALP}.
		\item Our results on the GRLP are the first to theoretically analyze the use of linear function approximation of Langrangian (dual) variables underlying the constraints.
		\item A numerical example in controlled queues is provided to illustrate the theory.
\end{enumerate}
A short and preliminary version of this paper without the theoretical analysis is available in \cite{aaaipaper}.
