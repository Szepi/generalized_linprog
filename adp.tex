%!TEX root =  autocontgrlp.tex
\section{Approximate Dynamic Programming: Objectives and Methods}
\FloatBarrier
\begin{table}[H]
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|}\hline
ADP Method	&Empirical	&Theoretical\\\hline
Projected Bellman &\ding{51}    &\ding{53}-Policy Chattering\\
Equation	&	&\\\hline
ALP		&\ding{53}-Large number of Constraints &\ding{51}\\\hline
RLP		&\ding{51} &\ding{53}-  Only under\\
&&ideal assumptions\\\hline
\end{tabular}
}
\end{table}

\begin{comment}
In practical problems, often the states are best described as multi-dimensional tuples.
In this context, the term \emph{curse-of-dimensionality} refers to the fact that the number of states
	grows exponentially as a function of the number of dimensions
	and thus solution methods whose computation cost grows at least linearly with the size of the state space
	quickly become intractable.
%Most MDPs arising in practical applications suffer from the curse, i.e., have large number of states and it is difficult to compute $J^*/J_u\in \R^n$ exactly in such scenarios. 
Approximate dynamic programming (ADP) \cite{lspi,lspe,ALP,wang2014approximate} methods compute an approximate value function $\tilde{J}$ instead of $J^*/J_u$. 
In order to tackle the curse, ADP methods resort to dimensionality reduction by parametrizing the value function and/or the policy. Value-function based ADP methods form an important subclass of ADP methods whose brief overview is given below.

\subsection{Value-Function based ADP}
In the value-function based ADP schemes a parametrized family is chosen and the approximate value-function $\tilde{J}$ is picked from the chosen parametrized class.
Once the approximate value function $\tilde{J}$ is computed, a sub-optimal policy $\tilde{u}$ can be obtained as the one-step greedy policy with respect to $\tilde{J}$ by making use of \eqref{subpol}.
The following lemma,
	bounds the degree of sub-optimality of the greedy policy $\tilde{u}$: 
	\todoc{Is this tight? Add a note.}
\begin{lemma}\label{subopt}
Let $\tilde{J}$ be the approximate value function and $\tilde{u}$ be as in \eqref{subpol}, then 
\begin{align}
||J^*-J_{\tilde{u}}||_\infty \leq \frac{2}{1-\alpha}||J^*-\tilde{J}||_\infty\,.
\end{align}
\end{lemma}
This lemma is easy to prove and has appeared multiple times in the literature (e.g., \cite{SinghYee94}).
	
The quality of any ADP method depends on the approximation guarantees it offers for the quantities $||J^*-\tilde{J}||$ and $||J^*-J_{\tu}||$, where $||\cdot||$ is an appropriate norm. The term  $||J^*-\tilde{J}||$ denotes the error in prediction and $||J^*-J_{\tu}||$ denotes the loss in performance resulting from the sub-optimal policy $\tu$ with respect to the optimal policy $u^*$. Of the two error terms, $||J^*-J_{\tu}||$ is more important because ultimately we are interested in coming up with a useful policy. In the context of ADP methods, the control and prediction problems are said to be addressed when the error terms $||J^*-\tilde{J}||$ and $||J^*-J_{\tu}||$ are both bounded by ``small'' constants.

\subsection{Linear Function Approximation}
The most widely used parametrization is linear. 
When such linear function approximators (LFAs) are employed,  
	the value-function is approximated as $\tilde{J}=\Phi r^*$, 
	with $\Phi=[\phi_1|\ldots|\phi_k]$ being an $n\times k$ feature matrix and $r^*$ is a
	parameter to be computed.

There are two widely used approaches to value function approximation. 
Both the approaches start out with a basic solution method and appropriately introduce function approximation in it. 
The two approaches are:
\begin{enumerate}
\item The Projected Bellman Equation (PBE) which combines the BE and the linear least squares projection operator to project high dimensional quantities onto the subspace of the LFA.
\item The approximate linear programming formulation which introduces the LFA in the linear programming formulation. 
\end{enumerate}
A host of ADP methods are based on the PBE and have been found to be useful in practice. 
The main application of the PBE is for approximate policy evaluation, 
i.e., to compute $\tilde{J}_u$, an approximation to the value function $J_u$ of policy $u$. 
Due to the mismatch in the norms, 
	i.e, the linear least squares projection operator based on the $L_2$-norm 
	and the $L_\infty$-norm of the Bellman operator $T$, 
	one cannot use the PBE to obtain a direct approximation to $J^*$. 
Thus in order to solve the problem of control, $\tilde{J}_u$ is used to compute a one-step greedy policy. 
However, again due to the mismatch in the norms, 
	i.e., $L_2$-norm of the linear least squares projection 
	and the $L_\infty$ norm required for policy improvement (Lemma~\ref{subopt}), 
	the one-step greedy policy need not necessarily be an improvement. 
	This leads to a phenomenon called \emph{policy-chattering} \cite{dpchapter} 
	where looping within of bad policies can occur. 
	Further, such policy-chattering can be demonstrated in simple examples as well \cite{dpchapter}. 
	Thus, though the approximate value function obtained by solving the PBE offers guarantees 
	for prediction it does not offer any guarantees for the control problem, 
	a significant shortcoming of the PBE based methods. 
	\todoc{Again, this needs to be changed as there are guarantees as mentioned in the intro.}

The ALP formulation \cite{ALP} on the other hand 
	does not suffer from issues such as policy-chattering, 
	for the simple reason that it computes $\tilde{J}$ 
	which is an approximation to $J^*$ 
	and a one-step greedy policy $\tilde{u}$ obtained using $\tilde{J}$. 
In short, since there is only one policy that the ALP outputs there is no question of policy-chattering. 
	\todoc{Policy chattering is annoying but the real problem is not policy chattering.
	If all policies in the long run would be pretty good, no one would care or should care about the lack of convergence.
	The problem is that to guarantee good performance one needs extra assumptions on the MDP,
	which may or may not hold in a given application.
	}
Further, ALP offers performance guarantees 
	for both the error terms $||J^*-\tilde{J}||$ and $||J^*-J_{\tilde{u}}||$. 
	\todoc{This is confusing. On one hand, bounding $||J^*-\tilde{J}||$ is enough based on Lemma~\ref{subopt}.
	On the other hand, alone I don't get why anyone would care about $\norm{J^* - \tilde{J}}$?
	Maybe to get a certificate of how well the policy will do (lower bound)? This should be explained at minimum.
	}
	Though the ALP is a complete method addressing 
		both the control and prediction problems, 
		it nevertheless suffers from an important limitation in the form of large number of constraints 
		(as large as the size of the state space). 
This limitation has been addressed in literature 
	by sampling only a fewer tractable constraints to formulate a reduced linear program (RLP). 
The RLP has been shown to perform well in practice \cite{ALP,CS,CST}, 
	but theoretical performance guarantees \cite{CS} are available for a specific RLP obtained under idealized assumptions. 
In this paper, by providing a sound theoretical analysis of the RLP, 
	we aim to show that RLP is a complete method that addresses both the prediction and the control problems.
We achieve this by developing and presenting 
	a comprehensive theoretical framework to understand the constraint reduction/approximation procedure. 
\todoc{We probably will need to discuss at some point other related work, such as
the ``Smoothed Linear Program'' due to Desai et al (which aims at the problem when the number of next states
is high and sampling is used instead of the Bellman operator in the definition of the LP),
Petrik and Pazis and Parr's approach to the dilemma of what features to use (L1 regularization,
non-parametric approximate LP's),
dual dynamic programming due to Wang et al.,
partitioned linear programming due to Kveton and Hauskrecht,
Wang and Bertsekas'  incremental constraint projection methods for variational inference,
Bertsimas and Caramanis' ``adaptability via sampling'',
Guestrin and Hauskrecht's ``Solving factored MDPs with continuous and discrete variables'' amongst others.}

In the next section, we discuss the approximate linear programming (ALP) formulation, the basic results and present prior results in literature as well as motivate the problem that we address in this paper. 
\end{comment}
