%!TEX root =  autocontgrlp.tex
\section{Generalized Reduced Linear Program}
In this section we present the generalized reduced linear program (GRLP) which is obtained by appropriately extending the definition of the RLP. An important property that should carry over from the RLP is that the feasible set of the GRLP should also be a superset of the feasible set of the ALP. A natural way to achieve this is to replace the set of sampled constraints in the RLP by a set of constraints which are obtained as linear combinations of the original constraints of the ALP. Formally, we define the generalized reduced linear program (GRLP) as below:
\begin{align}\label{grlp}
\min_{r\in \chi} &c^\top \Phi r,\nn\\
\text{s.t}\mb & W^\top E\Phi r\geq W^\top H \Phi r, 
\end{align}
where $W \in \R_+^{nd\times m}$ is an $nd\times m$ matrix with all nonnegative entries and $\chi \subset \R^k$ is any bounded set such that $\hj \in \chi$. \todoc{I don't think that $\chi$ is used in a consistent fashion. On the first reading though, I noticed this. Also, the role of $\chi$ is never explained. Or did I just miss this? Do we want to keep $\chi$ or maybe we should just remove it?}
Thus the $i^{th}$ ($1\leq i\leq m$) constraint of the GRLP is a positive linear combination of the original constraints of the ALP. Constraint reduction is achieved by choosing $m<<nd$. The key difference between the RLP in \eqref{rlpshort} and the GRLP in \eqref{grlp} despite their similar structure is that while $\M$ is a matrix of only zeros and ones, $W$ is a matrix of positive entries alone. Also note that an RLP is trivially a GRLP as well. Unless specified otherwise we use $\hr$ to denote the solution to the GRLP in \eqref{grlp}, $\hj=\Phi \hr$, to denote the corresponding approximate value function and $\hu$ to denote the greedy policy with respect to $\hj$.\par
Note that we want to avoid certain uninteresting and trivial cases of $W$ matrix such as $W=0$ or an entire column of $W$ being zero (which means no constraint is generated with respect to that column). Thus it is intuitive to demand that every column of $W$ should be non-negative and have at least one entry which is strictly positive. Also, normalizing columns of $W$ so that they sum to $1$ does not make any difference to the constraints of the RLP. Keeping these in mind we also assume the following throughout the rest of the paper:
\begin{assumption}\label{wassump}
$W \in \R^{nd\times m}_+$ is a full rank $nd\times m$  matrix (where $m<<nd$) and each of its column sums equals $1$.
\end{assumption}
The above assumption is just a technical condition that eliminates uninteresting choices such as $W=0$ or cases when certain columns of $W$ have all zeros, which implies that the corresponding column generates no constraint.
\input{cartoon}
The rest of the paper develops analytically various performance bounds and our main results provide the following:
\begin{enumerate}
\item A bound for $||J^*-\hj||$, the error between the approximate value function $\hj$ as computed by the GRLP and the optimal value function $J^*$;
\item a bound for $||J^*-J_{\hu}||$, the loss in performance due to the greedy policy $\hu$ measured with respect to the optimal policy; and
\item an important result on constraint sampling.
\end{enumerate}
We achieve the above via two novel $\max$-norm contraction operators namely the least upper bound (LUB) projection operator (denoted by $\Gamma$) and the approximate least upper bound (ALUB) projection operator (denoted by $\hat{\Gamma}$). We bound the error due to constraint approximation by analyzing the fixed points of the operators $\Gamma$ and $\hg$. We first establish our results in the $L_\infty$-norm and then extend the same in a modified $L_\infty$-norm. The schematic in Fig. ~\ref{schematic} provides a pictorial representation of what shall follow in the next three sections.
\FloatBarrier
\input{schematic}

