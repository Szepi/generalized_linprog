%!TEX root =  autocontgrlp.tex
\section{Main Results}
%We denote an arbitrary solution to GRLP by $\hr$, and the approximate value function by $\hj=\Phi \hr$ and use $\hu$ to denote the greedy policy w.r.t. $\hj$.\\


The purpose of this section is to present our main results.
\newcommand{\Jalpo}{J^*_{\alp}}
\newcommand{\Jlro}{J^*_{\lralpshort}}
\newcommand{\Jlr}{J_{\lralpshort}}
\newcommand{\rlr}{r_{\lralpshort}}
Let $\rlr$ be a solution to the LRALP given by \eqref{grlp}
and let $\Jlr = \Phi \rlr$. When multiple solutions exist, we can choose any of them.
For the result, we assume that the LRALP is not unbounded, and hence a solution exist. In fact, we will assume something much stronger. The discussion of why our assumptions are reasonable and how to ensure that they hold is postponed to after the presentation of our results.
Our main results bounds the error $\norm{J^* - \Jlr}_{1,c}$.

The bound is given in terms of the approximation error of $J^*$ with the basis functions $\Phi= (\phi_1,\dots,\phi_k)$, as well as the deviation between two functions, $\Jalpo,\Jlro: \S \to \Re$, which we define next. In particular,
\begin{align*}
\Jalpo(s) & = \min\{ r^\top\phi(s) \,|\, \Phi r \ge J^*, \, r\in \Re^k \}\,,\\
\Jlro(s)    & = \min\{ r^\top\phi(s) \,|\, W^\top E \Phi r \ge W^\top E J^*, \, r\in \Re^k \}\,,
\end{align*}
where $s\in \S$. Recall that $E: \Re^S \to \Re^{SA}$ is defined so that $(E J)^\top = (J^\top, \dots, J^\top)$, i.e., $E$ stacks its argument $A$-fold. Hence, $W^\top E = \sum_a W_a^\top$. Our strong assumption is that $\Jlro$ is finite-valued. Note that $\Jalpo\ge J^*$ reflects the error due to using the basis functions $(\phi_j)_j$, and the magnitude of the deviation $\Jlro-\Jalpo$ reflects the error introduced due to the relaxed constraint system. \todoc{I commented out the figure. We can discuss whether we want to include it.}
%The function $\Jalpo$ is an upper approximation to $J^*$

Following \citet{ALP,CS}, 
in the result the magnitude of the error $\Jlro-\Jalpo$ and also that of the error
of approximating $J^*$ with the subspace spanned by $\Phi$, 
will be measured in terms of a \emph{weighted maximum norm}, 
$\norm{J}_{\infty,\psi} = \max_{s\in \S} |J(s)|/\psi(s)$, 
where $\psi: \S \to \Re_{++}$ is a positive-valued weighting function.%
\footnote{As opposed to \citet{ALP} and others, our definition uses division and not multiplication with the weights.
We choose this form for mathematical convenience:
With this definition, nice duality results hold between weighted $1$-norms and weighted maximum norms.
}
As also stressed by \citeauthor{ALP}, 
the appropriate choice of $\psi$ is crucial for MDPs with huge state-spaces:
The problem is that if the range of values of $|J^*(s)|$ in different parts of the state space
differ in orders of magnitude, it is not meaningful to expect to control the error of approximating it uniformly.
By choosing the weighting function to reflect the magnitude of $J^*$, controlling the weighted maximum norm
is achieved by controlling relative errors, which may be much easier to ensure than controlling meaningfully small
absolute errors.

Just like \citet{ALP}, we will also require that $\psi$ is a \emph{stochastic Lyapunov-function} for the MDP.  In particular, we require that the $\alpha$-discounted stability coefficient
\begin{align*}
\beta_\psi \doteq \alpha  \max_{a} \norm{P_a \psi}_{\infty,\psi}
\end{align*}
is strictly less than one.
This can be seen to imply that $H: (\Re^S,\norm{\cdot}_{\infty,\psi}) \to (\Re^{SA}, \norm{\cdot}_{\infty,\psi})$ is a contraction,
where for $J = (J_1^\top,\dots,J_A^\top)^\top \in \Re^{SA}$ we let $\norm{J}_{\infty,\psi} = \max_a\norm{J}_{\infty,\psi}$.
That $H$ is a contraction will play a crucial role in our results.
Note that the condition $\beta_\psi<1$ is closely related to the condition that for any policy $u$, 
$P_u \psi \le \psi$, which can be viewed as a stability condition on the MDP 
and which appeared in a slightly altered form in studying the stability of MDPs with infinite 
state spaces \citep[e.g.,][]{chemey99a}.
Note that one can always choose $\psi = \one$: $\beta_\one = \alpha<1$. 
With this, we are ready to state our main result:


\if0
\FloatBarrier
%\input{cartoon}
\begin{figure}
\includegraphics[scale=0.7]{cartoon_grlp.pdf}
\caption{
%\normalsize
The outer lightly shaded region corresponds to GRLP constraints and the inner dark shaded region corresponds to the original constraints. The main contribution of the paper is to bound $\norm{J^*-\hat{J}_c}$.}
\label{cartoon}
\end{figure}
\Cref{cartoon} shows the solutions to the LP, ALP and GRLP respectively. The error in ALP solution has already been studied in \cite{ALP}. Our objective is to study the extra source of error due to constraint approximation.
\fi

%\subsection{Error Bounds}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}[Error Bound for LRALP]
\label{cmt2}
Assume that $c\in \Re_+^S$ is such that $1^\top c = 1$ and that $W \in \Re_+^{SA\times m}$ is nonnegative valued.
Let $\psi\in \Re_+^S$ be in the column span of $\Phi$ and assume that the $\alpha$-discounted stability coefficient of $\psi$ is $\beta_\psi<1$. 
Let $\eps = \inf_{r\in \Re^k}\norm{J^*-\Phi r}_{\infty,\psi}$ 
be the error of approximation $J^*$ using the basis functions in $\Phi$.
Then,
\begin{align*}
\norm{J^*-\hj}_{1,c} \leq 
 \frac{2 c^\top \psi}{1-\beta_\psi} \left(
	3 \eps
     +\norm{\Jalpo-\Jlro}_{\infty,\psi}\right)\,.
\end{align*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\if0
\item The control error is bound as\\
$\norm{J^* - J_{\hu}}_{1,c}
\leq 2\left(\frac{1}{(1-\alpha)^2}\right)\, \big( 2~\us{\min}{r\in \Re^k} \norm{J^*-\Phi r}_{\infty}
+\norm{\Gamma J^*-\hg J^*}_{\infty}+\norm{\hj-\hg\hj}_{\infty}\big)$.
\end{enumerate}
\fi
\end{theorem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Note that the result implicitly assumes that $\hj$ exists, because if $\hj$ does not exist then $\Jlro$ is necessarily unbounded, making the last error term infinite. To ensure that $\psi$ is in the span of $\Phi$, after choosing $\psi$, one can add $\psi$ as one of the basis functions. Alternatively, the bound can also be interpreted to hold for any $\psi$ in the span of $\Phi$ with $\beta_\psi<1$.

As noted earlier, \citet{ALP} prove a similar error bound for $\Jalp$, the solution of the ALP.
In particular, their Theorem 3 states that  under identical assumptions as in our result,
$\norm{J^* - \Jalp}_{1,c} \le \frac{2 c^\top \psi \epsilon}{1-\beta_\psi}$ for $\epsilon$ defined as above
 (the result we cited previously is a simplified form of this bound).
The larger coefficient of $\epsilon$ is probably an artifact of our analysis. 
Note that when $W$ does not reduce the constraints, 
our bound is only a constant factor larger than this previous result.
The extra term $\norm{\Jalpo-\Jlro}_{\infty,\psi}$ can be seen as the price paid for relaxing the constraints.

From linear programming theory, it follows that primal boundedness is equivalent to dual feasibility. 
Since the dual of $\min\{c^\top x\,:\, Ax \ge b \}$ is 
$\max\{ y^\top b\,:\, y \ge 0, c = A^\top y\}$, we get
that a necessary and sufficient condition for $\Jlro$ 
to be finite-valued is that for any $s\in \S$, $\phi(s)$ lies in the conic span of $U = \Phi^\top E^\top W$.
%(Note that the boundedness of LRALP only requires that $\Phi^\top c$ lies in the conic span of $U$.)
When $W$ is such that its constituents $W_1,\dots,W_A$ are all identical, 
the conic span of $U$ is equal to the conic span of $\Phi^\top W_1$. 
It is particularly instructive 
to consider the case when the common matrix $W_a = (w_{1,a},\dots,w_{m,a})$ ``selects'' the $m$ states,
i.e., when $\{w_{1,a},\dots,w_{m,a}\}= \{ e_{s}\,:\, s\in \S_0 \}$ for some $\S_0\subset \S$, $|\S_0|\le m$.
In this case the condition that $\phi(s)$ lies in the conic span of $U$ is equivalent to that $\phi(s)$ lies in the conic
span of $\{\phi(s')\,:\, s'\in \S_0\}$. 
Thus, to ensure boundedness, 
the chosen states should be selected to ``conicly cover'' $\phi(\S)\doteq \{\phi(s)\,:\,s\in \S\}$.

\todoc[inline]{TODO: 
Finish error bound (see theorem below).
Discuss how this can be done with common choices of basis functions.
State aggregation.
Separable choices.
Also, this conic cover is a bit stringent.
}

\begin{theorem}
Let $\phi(s)$%
\footnote{Here $\phi(s)$ is the $s^{th}$ row of the feature matrix $\Phi$} be the feature of the $s^{th}$ state. Let $i$ be a state whose constraints have not been retained and let $\{J\}$ denote the set of states whose constraints have been retained. Further, let $\phi(i)\in \Re^k$ lie in the linear span of vectors $\phi(j)\in \Re^k,\forall j\in \J$, then
\begin{align*}
\MoveEqLeft (\Gamma J^*-\hg J^*)(i)
 \leq 
(\Gamma J^*-J^*)(i) \\
& {}+J^*(i)-\underset{j\in \J}{\sum} |\gamma_j| J^*(j)
+ \underset{j\in \J}{\sum}(|\gamma_j|-\gamma_j) B,
\end{align*}
where $\gamma_j$ are the co-efficients of the linear span and $B\eqdef \underset{\underset{s\in S}{r\in \N}}{\max}(\Phi r) (s)$
\end{theorem}
\begin{proof}
Let $r^s_{\Gamma}$ and $r^s_{\hg}$ be such that $(\Phi r^s_{\Gamma})(s)=(\Gamma J^*)(s)$ and $(\Phi r^s_{\hg})(s)=(\hg J^*)(s)$
\begin{align*}
\MoveEqLeft (\Gamma J^*-\hg J^*)(i) 
=\phi(i) r^s_{\Gamma}- \phi(i) r^s_{\Gamma}\\
&=\phi(i) r^i_{\Gamma}- \underset{j\in \J}{\sum} (|\gamma_j|+\gamma_j -|\gamma_j|)\phi(j) r^i_{\hg}\\
&\leq \phi(i) r^i_{\Gamma}- \underset{j\in \J}{\sum} |\gamma_j| J^*(j) + \underset{j\in \J} {\sum} (|\gamma_j|-\gamma_j)B\\
&= \phi(i) r^i_{\Gamma}- J^*(i)+ J^*(i) -\underset{j\in \J}{\sum} |\gamma_j| J^*(j) \\
& \quad + \underset{j\in \J}{\sum} (|\gamma_j|-\gamma_j)B\,.
\end{align*}
\end{proof}



Note that the bound of \cite{CS} and our bound can be seen as largely complementary. 
Recall that \citeauthor{CS} consider adding an extra constraint $r\in \N$, while they propose to select all $A$ constraints
from the ALP corresponding to $m$ states chosen at random from some distribution $\mu$. 
Then, with high probability,
they show that, provided that $\ralp \in \N$,
 the extra price paid for relaxing the constraints of the ALP is $O( \rho \epsilon_{\N} k/m)$,
 where $\rho = \max_{s} \frac{\mu^*(s)}{\mu(s)}$, $\mu^* = (1-\alpha)c^\top (I-\alpha P_{u^*})^{-1}$, $u^*$ is an optimal policy,
and $\epsilon_{\N} = \sup_{r\in \N} \norm{J^*-\Phi r}_{\infty,\psi}$.%
\footnote{The paper presents the results for $\mu = \mu^*$ giving $\rho=1$, but the analysis easily extends to the general case.}
The bound is nontrivial when $m\ge \rho \epsilon_{\N} k$.
In general, it may be hard to control $\rho$, or even $\epsilon_{\N}$ while ensuring that $\ralp \in \N$.



\if0
\textbf{On Prediction Error:} The first factor in the right hand side of the prediction error in \Cref{cmt2} is related to the best possible approximation that can be achieved with the chosen feature matrix $\Phi$. This term is an carry over of the upper bound in ALP formulation as shown in \Cref{alpvanilla}. The second factor in the right hand side of the prediction error is related to constraint approximation and is completely defined in terms of $\Phi$, $W$ and $T$, and does not require knowledge of stationary distribution of the optimal policy.\par
\textbf{On Control Error:} The first two terms are quite similar to those in the bound for prediction error. The third term occurs due to the fact that $\Phi \geq T\Phi r$ that holds in the case of ALP does not hold in the case of GRLP.\par
\begin{comment}
\begin{theorem}[Control Error Bound in $\norm{\cdot}_{\infty}$]
\label{polthe}
Let $\hu$ be the greedy policy with respect to the solution $\hj$ of GRLP and $J_{\hu}$ be its value function.
% Let $r^*$ be as in Theorem~\ref{mt2mn}, then
Then,
\begin{align}\label{polthebnd}
\norm{J^* - J_{\hu}}_{1,c}
&\leq 2\left(\frac{c^\top \psi}{(1-\beta_{\psi})^2}\right)\, \big( 2\norm{J^*-\Phi r^*}_{\infty}
\nn\\&
+norm{\Gamma J^*-\hg J^*}_{\infty}+\norm{\hj-\hg\hj}_{\infty}\big).
\end{align}
\end{theorem}
\end{comment}
\begin{corollary}[Constraint Sampling]\label{st}
$W\in \{0,1\}^{nd\times m}$Let $s\in S$ be a state whose constraint is selected by $W$ (i.e., for some $i$ and all $(s',a)\in S\times A$,
$W_{s'a,i}=\delta_{s=s'}$.
Then
\begin{align*}%\label{sampexp}
|\Gamma J^*(s)-\hg J^*(s)|<|\Gamma J^*(s)-J^*(s)|.
\end{align*}
\end{corollary}
It is important to note that \Cref{rlpt} holds only in high probability and is valid only under idealized assumption of knowing the optimal policy $u^*$, while \Cref{cmt2} does not have these limitations.
In addition, the error $|\Gamma J^*(s) -\hg J^*(s)|$ (in \Cref{st} ) due to constraint approximation is less than the original projection error $|\Gamma J^*(s)-J^*(s)|$ due to function approximation. This means that for RLP to perform well it is important to retain the constraints corresponding to those states for which the linear function approximation via $\Phi$ is known to perform well.
\fi

