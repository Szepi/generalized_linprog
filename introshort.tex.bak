%!TEX root =  autocontgrlp.tex
\section{Introduction}\label{intro}
\begin{comment}
Markov decision processes (MDPs) is an important mathematical framework to study optimal sequential decision making problems that arise in science and engineering. Solving an MDP involves computing the optimal \emph{value-function} ($J^*$), a vector whose dimension is the number of states. MDPs with small number of states can be solved easily by conventional solution methods such as value/ policy iteration or linear programming (LP) \cite{BertB}. However, it is difficult to compute  $J^*$ exactly when there are a large number of states. A practical way to tackle the curse is to empoly function approximation, i.e., to choose an approximate value function $\tilde{J}$ instead of $J^*$ from a paramterized function class. Linear function approximation is the most common, wherein, the value function is approximated as $J^*\approx \tj=\Phi r^*$, where $\Phi$ is a feature matrix whose columns are the basis functions and $r^*$ is a weight vector to be learned. A representational advantage (in terms of the number of unknowns) is achieved by choosing fewer number of basis functions compared to the number of states. Once approximate value function $\tilde{J}$ is known, a sub-optimal policy $\tilde{u}$ which is greedy with respect to $\tilde{J}$ can be computed.  The quality of approximation depends on the quantity $||J^*-\tilde{J}||$. 
\end{comment}
Optimal sequential decision making problems occurring in science, engineering and economics can be cast in the framework of Markov Decision Processes (MDPs), where the problem is to find a policy $u$, mapping states to actions, so as to maximize long term expected cumulated discounted reward. A given policy $u$ is associated with a value function $J_u$ (a mapping from the state space to the set of reals\footnote{Also known as reward/cost-to-go function.}), which gives the value of each state under a given policy $u$. The optimal value function $J^*$ collects the highest values achievable in each state. A policy $u^*$ is optimal if it achieves the optimal value in each state, i.e., if $J^* = J_{u^*}$. Conventional solution methods for Markov Decision Processes (MDPs) \cite{BertB} such as value iteration, policy iteration and linear programming formulation compute the optimal value function and the optimal policy. However, computing the exact quantities for each and every state is hard when there the MDP has large number of states.\par
A practical way to address the issue of large number of states is function approximation, where, the idea is to choose a function belonging to a parameterized family of functions as an approximation to the exact value function. Linear function approximation is the most common, wherein, the value function is approximated as $J^*\approx \tj=\Phi r^*$, where $\Phi$ is a feature matrix whose columns are the basis functions and $r^*$ is a weight vector to be learned. A representational advantage (in terms of the number of unknowns) is achieved by choosing fewer number of basis functions compared to the number of states. Once approximate value function $\tilde{J}$ is known, a sub-optimal policy $\tilde{u}$ which is greedy with respect to $\tilde{J}$ can be computed. The approximation error is quantified by the terms $||J^*-\tj||$ which is the error in approximating the value function and $||J^*-J_{\tilde{u}}||$ which is the loss in performance due to the sub-optimal policy $\tilde{u}$ in comparison to the performance of the optimal policy.\par
The \emph{approximate linear programming} (ALP) \cite{ALP,CS,SALP,ALP-Bor,gkp,fs,npalp} host of methods introduce linear function approximation in the linear programming formulation. 
%The ALP method outputs an approximate value function $\tj$ and a sub-optimal policy $\tu$ which is obtained as a greedy policy with respect to $\tj$. The approximation error $||J^*-\tilde{J}||$ and the loss in performance $||J^*-J_{\tilde{u}}||$ can be bounded \cite{ALP}. 
A critical shortcoming of ALP is that the number of constraints are of the order of the state space, making, in the lack of extra structure, the vanilla version of ALP intractable. One proposal in the literature to overcome this hurdle is to employ a procedure known as constraint sampling, wherein a subset of the original constraints of the ALP are sampled to formulate a \emph{reduced linear program} (RLP). The perfromance analysis of the RLP can be found in \cite{CS} and the RLP has also been shown to perform well in experiments \cite{ALP,CS,CST}. %However, theoretical results are available only for a specific RLP 	formulated under idealized conditions \cite{CS}. 
An alternative approach to handle the constraints is employ function approximation in the dual variables of the ALP \cite{ALP-Bor,dolgov}. However, \cite{ALP-Bor,dolgov} does not provide theoretical guarntees for the loss in performance due to such an approximation.\par 
The success of the RLP \cite{CS,CST} and the idea of approximating the dual variables \cite{ALP-Bor}, naturally leads us to the question understanding linear function approximation of the constraints. To this end, in this paper, we generalize the RLP to define a generalized reduced linear program (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints of the ALP. 
The salient aspects of our contribution are listed below:
\begin{enumerate}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$ (\Cref{cmt2mn}). 
		\item We also bound (Theorem~\ref{polthe}) the loss $||J^*-J_{\hu}||$ of the one-step greedy policy $\hu$ based on $\hj$.
%		\item Akin to the error analysis in \cite{ALP,CS,SALP} our bounds are also in terms of a weighted $L_\infty$-norm.
		\item Our analysis is based on a novel $\max$-norm contraction operator  and our results hold with probability one. This is another significant difference in comparison to the results on constraint sampling in \cite{SALP,CS} that make use of concentration bounds and hold only with \emph{high} probability.
		\item The structure of the error terms also reveals that it is not always necessary to sample using the stationary distribution of the optimal policy.
		\item Our results on the GRLP are the first to theoretically justify linear function approximation of the constraints.
\end{enumerate}
