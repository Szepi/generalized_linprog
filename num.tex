%!TEX root =  autocontgrlp.tex
\section{Numerical Illustration}
In this section, we show via an example in the domain of controlled queues to show that the consequences of \Cref{conetheorm} (the \emph{cone} condition) derived based on the geometric intuition (please refer \cite{aaaipaper} for preliminary experiments in this domain).

\textbf{Queuing System Model:} The queuing model used here is similar to the one in Section~$5.2$ of \cite{ALP}. We consider a single queue with arrivals and departures. The state of the system is the queue length with the state space given by $S=\{0,\ldots,n-1\}$, where $n-1$ is the buffer size of the queue. The action set $A=\{1,\ldots,d\}$ is related to the service rates. We let $s_t$ denote the state at time $t$. The state at time $t+1$ when action $a_t \in A $ is chosen is given by $s_{t+1}= s_{t}+1$ with probability $p$, $s_{t+1}= s_{t}-1$ with probability $q(a_t)$ and $s_{t+1}= s_t$, with probability $(1-p-q(a_t))$. For states $s_t=0$ and $s_t=n-1$, the system dynamics is given by $s_{t+1}= s_{t}+1$ with probability $p$ when $s_t=0$ and $s_{t+1}=s_t-1$ with probability $q(a_t)$ when $s_t=n-1$. The service rates satisfy $0<q(1)\leq \ldots\leq q(d)<1$ with $q(d)>p$ so as to ensure `stabilizability' of the queue. The reward associated with the action $a \in A$ in state $s\in S$ is given by $g_a(s)=-(s/n+q(a)^3)$.\par
\textbf{Parameter Settings:} We ran our experiments $n=1000$, $d=4$ with $q(1)=0.2$, $q(2)=0.4$, $q(3)=0.6$, $q(4)=0.8$, $p=0.4$ and $\alpha=1-\frac{1}{n}$.
The moderate size of $n=1000$ enabled us to compute the exact the exact value of $J^*$ (it was however expensive). We made use of polynomial features in $\Phi$ (i.e., $1,s,\ldots,s^{k-1}$) since they are known to work reasonably well for this domain \cite{ALP}, and chose $k=4$ (i.e., we used $1, s,s^2$ and $s^3$ as basis vectors).\\
\textbf{Experimental Methodology:} We compare two different sampling strategies $(i)$ based on the \emph{cone} conditions, and $(ii)$ based on constraint sampling. The two strategies are compared via  \emph{lookahead} policies, wherein, the action at state $s$ is obtained by computing the approximate value functions of the next states. The details are as follows:
$Case (i):$ Except for the corner states i.e., $s=1$ and $s=999$, each state $1<s<n-1$ has two next states namely $s'=s-1$ and $s'=s+1$. We formulate two separate LRALPs (or just one LRALP for $s=1$ and $s=n-1$) for next states. When formulating the LRALP for state $s'$, we let $c=e_{s'}$ and choose the constraint corresponding to state $s'$ to ensure the cone condition. We choose $5$ more constraints corresponding to states $1,200,400,600,800,999$ (uniformly spaced across the state space) and compute $\hat{J}_{e_s'}$. The lookahead policy is formulated as $u(s)=\underset{\arg\min}{a\in A}(g_a(s)+\underset{s'\in S}{\sum}p_a(s,s')\hat{J}_{e_s'}$.\\
$Case (ii):$ In a manner similar to $Case (i)$, here too we formulate two separate RLPs for next states. When formulating the RLP for state $s'$, we first let $c_{s'}(i)=(1-\alpha)(\alpha)^{|s'-i|}$ and normalize $c$ and sample $m=6$ constraints using $c$ and compute $\hat{J}_{c_s'}$. The lookahead policy is formulated as $u(s)=\underset{\arg\min}{a\in A}(g_a(s)+\underset{s'\in S}{\sum}p_a(s,s')\hat{J}_{c_s'}$.
