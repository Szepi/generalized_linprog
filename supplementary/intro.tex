%!TEX root =  autocontgrlp.tex
\section{Introduction}
Markov decision processes (MDPs) have proved to be an indispensable model for sequential decision making under uncertainty with applications in networking, traffic control, robotics, operations research, business, finance, artificial intelligence, health-care and more (see, e.g., \cite{
White93:Apps,
rust96:book,
FeiSh02:MDPHandbook,
QiWu07,
SiBu10:MDPinAI,
BauRie:11,Puter,
LeLiu12:RLBook,
Abuetal15:MDPWireless,
BouDi17:MDPPractice}).
In this paper we adopt the framework of discrete-time, discounted MDPs when
a controller steers the stochastically evolving state of a system while receiving 
rewards that depends on the states visited and actions chosen. The goal is to choose the actions so as to maximize the \emph{return}, defined as the total discounted expected reward. A controller that uses past state information is called a \emph{policy}. An \emph{optimal policy} is one that maximizes the value no matter where the process is started from \cite{Puter}.
In this paper we consider planning problems where the goal is to calculate actions of policies that give rise to high values
 and give new error bounds on the quality of solutions obtained by solving linear programs of tractable size. 
To explain the contributions in more details, we start by describing the computational challenges involved in planning.

The main objective of \emph{planning} is to compute actions of an optimal policy while interacting with an MDP model. 
In finite state-action MDPs,
assuming access to individual transition probabilities and rewards along transitions, 
various algorithms are available to perform this computation in time and space that scales polynomially with the number of states and actions.
However, in most practical applications, the MDP is compactly represented
and if it is not infinite, the number of states scale \emph{exponentially} with the \emph{size of the representation} of the MDP.
%an effect that is known as \emph{Bellman's curse of dimensionality}.
If planners are allowed to perform some fixed amount of calculations for each state encountered,
it is possible to use sampling to make the per-state calculation-cost 
independent of the size of the state space \cite{rust96:randomization,szepesvari2001,kearns2002sparse}.
\todoc{Maybe we can cite Mausam's book here as discussing a whole range of methods originating from AI
that target this problem, although without theoretical guarantees.}
Nevertheless, the resulting methods are still quite limited. 
In fact, various hardness results show that computing actions of (near-) optimal policies is intractable in various senses
and in various compactly represented MDPs \cite{BlonTsi:00Complexity}.
Given these negative results, 
it is customary to adopt the \emph{modest goal of efficiently computing actions of a policy that 
is nearly as good as a policy chosen by a suitable (computationally unbounded, and well-informed) oracle
from a given restricted policy class}. \todoc{We should probably elaborate on the oracle idea space permitting.
Or in the future. Btw, the situation is similar to statistical learning theory: There competing with the best choice 
is only information theoretically possible. And competing with best choice
in hindsight is often computationally intractable. Maybe cite information theory result for competing with the best policy in a class?}
Here, within some restrictions (see below), the policy class can be chosen by the user.
The more flexibility the user is given in this choice, the stronger a planning method is. \todoc{Add reference to the intro to Mausam's book. But where and what can be said?}

A popular approach along these lines, which goes back to \citet{SchSei85}, 
relies on considering linear approximations to the \emph{optimal value function}:
The idea is that, similarly to linear regression, a fixed sequence of basis functions are combined
linearly. The user's task is to use a priori knowledge of the MDP 
to choose the basis functions so that 
a good approximation to the optimal value function will exist in the linear space spanned by the basis functions.
The idea then is to design some algorithm to find the coefficients of the basis functions that gives a a good approximation, 
while keeping computation cost in check.
Finding a good approximation is sufficient, since at the expense of an extra $O(1/\epsilon^2)$ randomized computation, 
a uniform $O(\epsilon)$-approximation 
to the optimal value function can be used to calculate an action of an $O(\epsilon)$-optimal policy at any given state
(e.g., follow the ideas in \cite{szepesvari2001,kearns2002sparse}; see also Theorem 3.7 of \citet{Kall17}).
Since the number of coefficients can be much smaller than the number of states, the algorithms that search
for the coefficients have the potential to run efficiently regardless of the number of states.


Following \citet{SchSei85}, most of the literature considers
algorithms that are obtained from restricting exact planning methods to search 
in the span of the fixed basis functions when performing computations.
In this paper we consider the so-called \emph{approximate linear programming} (ALP) approach, which
was heavily studied during the last two decades, e.g.,
\cite{
schuurmans,
gkp,
ALP,
CS,
kveton2004heuristic,
petrik,
SALP,
fs,
npalp,
BhatFaMo12:SALPNP,
abbasi}.
The basic idea here is to combine a linear program whose solution is the optimal value function (and thus the number of optimization variables in it scales with the number of states) with a linear constraint that restricts the optimization variables to lie in the subspace spanned by the basis functions. As already noted by \citet{SchSei85}, the new LP can still be kept feasible by just adding one special basis function, while by substituting the ``value function candidates'' with their linear expansions, the number of optimization variables becomes the number of basis functions. 
As shown by \citet{ALP}, the solution to the resulting LP is within a constant factor of the best approximation to the optimal value function within the span of the chosen bases. However, since the number of constraints in the LP is still proportional to the number of states, it is not obvious whether a solution to the resulting LP can be found in time independent of the number of states (other computations can be done in time independent of the number of states, e.g., using sampling, at the price of a controlled increase of the error, e.g., Theorem 6 of \citep{petrik}).

Most of the literature is thus devoted to designing methods to select a tractable subset of the constraints while keeping the approximation guarantees, as well as keeping computations tractable. 
Since a linear objective is optimized by a point on the boundary of the feasible region, 
knowing the optimizer would be sufficient to eliminate all but 
as many constraints as the number of optimization variables.
The question is how to find a superset of these, or an approximating set, without incurring much computational overhead.
\citet{schuurmans} and \citet{gkp} propose constraint generation in a setting 
where the MDP has additional structure (i.e., factorized transition structure).
This additional structure is then exploited in designing constraint generation methods which are able to efficiently generate
violated constraints. A more general approach due to \citet{CS} 
is to choose a random subset of the constraints 
by choosing states to be included at random from a distribution that reflects the ``importance'' of states.
While constraint generation can be powerful,
it is not known how solution quality degrades with the budget on the constraints generated
(\citeauthor{gkp} note that the number of constraints generated can be at most exponential in a fundamental quantity,
the induced width of a so-called cost-network, which may be large and is in general hard to control).
For constraint sampling,  \citet{CS} prove a bound on the suboptimality, but this bound applies only 
in the unrealistic scenario when the constraints are sampled from an \emph{idealized} distribution,
which is related to the stationary distribution of an optimal policy. 
While it is possible to extend this result
to any sampling distribution, the bound then scales with the mismatch between the sampling and the idealized
distributions, which, in general, will be uncontrolled. 
Another weakness of the bound is related to that when constraints are dropped, the linear program may become
unbounded. To prevent this, \citet{CS}  propose imposing an extra constraint on the optimization variables.
The bound they obtain, however, scales with the \emph{worst approximation error} over this constraint set.
While in a specific example it is shown that this error can be controlled, no general results are derived in this direction.
Later works, such as that of
\citet{SALP,BhatFaMo12:SALPNP},  repeat the analysis of \citet{CS}
in combinations with other ideas. However, no existing work that we know of addresses the above weaknesses of the result of \citet{CS}. \todoc{Yasin's paper should be cited here as an interesting alternative approach. My notes about his paper:
Approximation in the dual. No control for infeasibility of resulting ALP. 
Algorithm: Stochastic gradient descent for computing near-optimal solution on Langrangian with large Langrangian coefficients.
Directly bounding loss of policy return as compared to best policy amongst those feasible. 
For infeasible LP, result is vacuous (the error blows up when demanding a better solution).
No bound on best feasible policy's return.
Strong requirements: Result depends on ability to choose a sampling distributions to control a variance term. 
"Few predecessor state-action pairs, which can be accessed, or sparse features", mixing. 
}

Our main contribution is a new suboptimality bound for the case when the constraint system is replaced with a smaller, 
linearly projected constraint system. We also propose a specific way of adding the extra constraint to keep the resulting LP bounded.
Rather than relying on combinatorial arguments (such as those at the heart of \citet{CS}), our argument uses previously unexploited geometric structure of the linear programs underlying MDPs.
As a result our bound avoids distribution-mismatch terms and we also remove the scaling with worst approximation error.
A specific outcome of our general result is the realization that it is beneficial to select states so that the ``feature vectors'' of all states when scaled with a fixed constant factor are included in the conic hull of the ``feature vectors'' underlying the selected states. This suggests to choose the basis functions so that this property can be satisfied by selecting only a few states. As we will argue, this property holds for several popular choices of basis functions.
A preliminary version of this paper without the theoretical analysis and without the geometric arguments was published in a short conference communication \cite{aaaipaper}. \todoc{This could be a footnote on page 1 if space works out better that way.}

\if0

The  framework of Markov decision processes (MDPs) is useful to mathematically cast optimal sequential decision making problems arising in science and engineering. At any decision instance, an action is made which yields an immediate reward and the system moves to the next state in a stochastic manner such that the next state depends only on the current state and the action chosen. The set of all states, the state space, is denoted by $S$, and the set of all actions, the action space, is denoted by $A$. Formally, a decision rule is called a policy $u$, ($u\colon S\ra A$) and has an associated value function $J_u$\footnote{Without loss of generality value function $J_u$ can be thought of as a vector in $\Re^{|S|}$.}, ($J_u(s)\colon S\ra \Re$) which specifies the expected cumulative reward obtained by following the policy $u$ starting from each state.\par
The so-called dynamic programming (DP) methods \cite{BertB} compute the optimal value function $J^*$ first and then obtain an optimal policy $u^*$ using $J^*$\footnote{Obtaining $u^*$ from $J^*$ is computationally cheap}. Conventional DP  techniques, such as value-, or policy-iteration, or linear programming (LP) \cite{BertB} can compute the exact value of $J^*$ (and $u^*$). However, a shortcoming of these conventional methods is that their computational overhead grows with the number of states, a practical hindrance when the MDP has a large number of states.\par
This paper is related to LP based techniques for MDPs with large state spaces. One way to handle the large number of states is to restrict the value function to the sub-space spanned by the columns of an $n\times k$ feature matrix $\Phi$\footnote{This is known as linear function approximation (LFA) where in the value function is approximated by $\Phi \tr$, for some $\tr\in \Re^k$. The idea of LFA is not restricted to LP based approach and is also widely used in other approximate dynamic programming methods \cite{dpchapter}, which are not discussed in this paper.}. This sub-space restriction can be accommodated in the LP formulation and results in the approximate linear programming (ALP) formulation \cite{ALP,CS,SALP,ALP-Bor}. ALP computes an approximate value function $\tj=\Phi \tr$ and a sub-optimal policy $\tu$ can be obtained using $\tj$\footnote{It is computationally cheap to obtain such a $\tu$ from $\tj$ or $u^*$ from $J^*$}. The sub-optimal policy can then be used to make the decision and results in a cumulative return given by $J_{\tu}$. The performance of ALP was studied in \cite{ALP} in terms of the quantities $\norm{J^*-\tilde{J}}$ and $\norm{J^*-J_{\tu}}$ ($\norm{\cdot}$ is an appropriate norm) known as prediction error and the control error respectively. Here, prediction error is the error in the approximate value function $\tj$ and the control error is the loss in performance due to the sub-optimal policy $\tu$.\par
%The \emph{approximate linear program} (ALP) \cite{ALP,CS,SALP,ALP-Bor} and its variants introduce linear function approximation in the linear programming formulation.
A critical shortcoming of ALP is that the number of constraints are of the order of the size of the state space, making it intractable in MDPs with large number of states. A way out of this shortcoming is to choose a subset of constraints at random and drop the rest, thereby formulating a \emph{reduced linear program} (RLP). The performance analysis of RLP can be found in \cite{CS} and RLP has also been shown to perform well in experiments \cite{ALP,CS,CST}. An alternative approach to handle the issue of large number of constraints is to employ function approximation in the dual variables of ALP \cite{ALP-Bor,dolgov}, an approach that was also found useful in experiments. However, to this date, there exist no theoretical guarantees for this approach.\par
In this paper, we generalize RLP to define a generalized reduced linear program (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints.
The salient aspects of our contribution are listed below:
\begin{enumerate}
\item We develop novel analytical machinery to relate $\hat{J}$, the solution to GRLP, and the optimal value function $J^*$ by bounding the prediction error $\norm{J^*-\hj}$ (\Cref{cmt2mn}).
\item We also bound the performance loss due to using the policy $\hu$ that is obtained using $\hj$ (Theorem~\ref{polthe}).
\item Our analysis is based on two novel $\max$-norm contraction operators and our results hold \emph{deterministically}, as opposed to the results on RLP \cite{SALP,CS}, where the guarantees have a probabilistic nature.
\item Our analysis also makes use of arguments based on \emph{Lyapunov} function, an approach much similar to prior works in ALP literature \cite{ALP,SALP}.
\item Our results on GRLP are the first to theoretically analyze the use of linear function approximation of Lagrangian (dual) variables underlying the constraints.
\item A numerical example in controlled queues is provided to illustrate the theory.
\end{enumerate}
A short and preliminary version of this paper without the theoretical analysis can be found in \cite{aaaipaper}.
\begin{comment}
\section{Introduction}
Markov decision processes (MDPs) are a powerful mathematical framework to study optimal sequential decision making problems  arising in science and engineering. In an MDP, the configuration of the system, the state, evolves in a stochastic manner in a way that the next state depends only on the last state and the action chosen. The set of all states, the state space, is denoted by $S$, and the set of all actions, the action space, is denoted by $A$.
%An instance of an MDP is a controlled queue setting, where there is a cost associated with the number of customers in the queue, and the aim is to control the service level depending on the number of %customers so as to achieve a minimum cumulative cost. In more general terms, given any MDP,
An optimal policy $u^*$ is a map from $S$ to $A$ that results in the best cumulative reward that can be obtained by starting from any state. The rewards for the various The so-called dynamic programming (DP) methods first compute what is known as the optimal \emph{value-function} ($J^*$), a vector whose dimension is the number of states, and use it to compute $u^*$. When the number of state is small conventional DP techniques, such as value-, or policy-iteration, or linear programming (LP) can be used to compute $J^*$ and $u^*$\cite{BertB}.\par
Curse-of-dimensionality  refers to the fact that the number of states grows exponentially in the number of state variables. Many practical systems such as controlled queues, inventory control etc are
afflicted by the curse, i.e., have large number of states. In such scenarios, it becomes increasingly difficult to compute the exact values of $J^*$ and $u^*$ because the DP methods are based on computations involving as many (or even more) number of variables as the number of states. A practical solution then is to compute an approximate value function $\tilde{J}$ instead of $J^*$. Approximate dynamic programming (ADP) methods combine an approximation architecture to represent $\tj$ and a conventional DP method to compute $\tj$. Eventually, ADP methods output a sub-optimal policy $\tu$ using the $\tj$ they compute. %Here, success depends on the quality of approximation, i.e., on the quantity $||J^*-\tilde{J}||$ for an appropriately chosen norm.\par
Linear function approximation (LFA), i.e., letting $\tilde{J}=\Phi \tr$ where $\Phi \in \Re^{|S|\times k}$ is a so-called feature matrix and $\tr*\in \Re^k$ is a weight vector (to be computed), is the most widely used method of approximation. Here, dimensionality reduction is achieved by choosing $\Phi$ to have fewer columns in comparison to the number of states ($k<<|S|$), holding the promise of being able to work with MDPs regardless of the number of states.\par
It is natural to expect that approximations lead to errors and it is important to quantify the errors. For a given ADP method, theoretical performance analysis analytically bounding the error terms $||J^*-\tilde{J}||$  and $\norm{J^*-J_{\tu}}$ which denote the error in approximating the value function, and performance loss due to following policy $\tu$ (here $J_{\tu}$ is the value of $\tu$) respectively. Further, in most cases the error terms reveal some structure that can offer insights and act as guide to the designer of the ADP method (for example the choice of $\Phi$). \par
%While many ADP methods use LFA, not all of them are successful. For instance, ADP methods that use linear least squares projection are known to exhibit `policy oscillations' \cite{dpchapter}, i.e., %output a repeating sequence of bad sub-optimal policies.
%Such an analysis is important to establish that the error is always bounded.
%Further, in most cases the error terms reveal some structure that can offer insights and act as guide to the designer of the ADP method (for example the choice of $\Phi$). \par
The \emph{approximate linear program} (ALP) \cite{ALP,CS,SALP,ALP-Bor} and its variants introduce LFA in the linear programming formulation to dynamic programming. Theoretical performance analysis of  ALP can be found in \cite{ALP}.
%and a salient feature is that it does not suffer from issues such as `policy oscillations'\footnote{ADP methods that use linear least squares projection are known to exhibit `policy oscillations' %\cite{BertB}, i.e., output a repeating sequence of bad sub-optimal policies. Since our focus in this paper is ALP formulation, we refrain from a detailed presentation of the other methods.}
The number of variables of ALP is only $k$, a critical shortcoming of ALP is that the number of constraints are of the order of the size of the state space, making it intractable.
Two approaches have been found empirically successful \cite{CS,dolgov,ALP-Bor} in addressing the issue of large number of constraints in ALP. In the first approach \cite{CS}, a random subset of constraints is chosen (dropping the rest), thereby formulating a \emph{reduced linear programming} (RLP) problem. The performance analysis of RLP can be found in \cite{CS}, however, the bounds hold only in high probability under idealized assumptions. The second approach involves employing function approximation in the dual variables of ALP \cite{ALP-Bor,dolgov}. However, to this date, there exist no theoretical guarantees bounding the loss in performance due to this approach.\par
Our motivation stems from the fact that ALP with tractable number of constraints will result in a full dimeniosnality free ADP method. However, constraint reduction in ALP is an extra source of error (in addition to the error due to LFA), which has not been theoretically well understood.  The focus of this paper is to fill this gap in theory by deriving performance bounds for constraint reduction in ALP formulation.
The salient aspects of our contribution are listed below:
\begin{enumerate}
\item We define a generalized reduced linear programming (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints. The GRLP amounts to linear function approximation of the dual variables, and RLP is a special case of GRLP.
\item We develop a novel analytical machinery to bound the prediction error $\norm{J^*-\hj}$ where $hj$ is the solution to GRLP.
		\item We also bound the performance loss due to using the policy $\hu$ that is one-step greedy with respect to $\hj$ (Theorem~\ref{polthe}).
		\item Our analysis is based on two novel $\max$-norm contraction operators and our results hold \emph{deterministically}, as opposed to the results on RLP \cite{SALP,CS}, where the guarantees have a probabilistic nature.
%Our analysis also makes use of arguments based on \emph{Lyapunov} functions, an approach much similar to prior works in ALP literature \cite{ALP,SALP}.
\item Our results on GRLP are the first to theoretically analyze the use of linear function approximation of Lagrangian (dual) variables underlying the constraints.
\item A numerical example in controlled queues is provided to illustrate the theory.
\end{enumerate}
%ALP with tractable number of constraints would result in  a full dimensionality free ADP method, without issues such as policy oscillations.
A short and preliminary version of this paper without the theoretical analysis is available in \cite{aaaipaper}. The rest of the paper is organized as follows:
\end{comment}
\fi