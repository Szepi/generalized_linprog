\section{Introduction}
Markov decision processes (MDPs) is an important mathematical framework to study optimal sequential decision making problems that arise in science and engineering. Solving an MDP involves computing the optimal \emph{value-function} ($J^*$), a vector whose dimension is the number of states. MDPs with small number of states can be solved easily by conventional solution methods such as value / policy iteration or linear programming (LP) \cite{BertB}. Dynamic programming is at the heart of all the conventional solution methods for MDPs.\par
A practical way to tackle the issue of large number of states is to compute an approximate value function $\tilde{J}$ instead of $J^*$. Here, success depends on the quality of approximation, i.e., on the quantity $||J^*-\tilde{J}||$. The most widely used method for approximation is the linear function approximation (LFA), i.e., let $\tilde{J}=\Phi r^*$, where $\Phi$ is a feature matrix and $r^*$ is a learnt weight vector. Dimensionality reduction is achieved by choosing $\Phi$ to have fewer columns in comparison to the number of states and this makes computing $\tilde{J}$ easier.\par
The \emph{approximate linear program} (ALP) \cite{ALP,CS,SALP,ALP-Bor} and its variants introduce linear function approximation in the linear programming formulation. A critical shortcoming of the ALP is that the number of constraints are of the order of the size of the state space, making the vanilla version of the ALP intractable. A way out is to employ a procedure known as constraint sampling, wherein a subset of the original constraints of the ALP are sampled to formulate a \emph{reduced linear program} (RLP). The performance analysis of the RLP can be found in \cite{CS} and the RLP has also been shown to perform well in experiments \cite{ALP,CS,CST}. An alternative approach to handle the issue of large number of constraints is to employ function approximation in the dual variables of the ALP \cite{ALP-Bor,dolgov}, an approach useful in practice, however, without any theoretical guarantees bounding the loss in performance resulting from such an approximation.\par
In this paper, we generalize the RLP to define a generalized reduced linear program (GRLP) which has a tractable number of constraints obtained as positive linear combinations of the original constraints of the ALP. 
The salient aspects of our contribution are listed below:
\begin{enumerate}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$ by bounding the prediction erorr $||J^*-\hj||$ (\Cref{cmt2mn}). 
		\item We also bound the performance loss due to the one-step greedy policy $\hu$ (greedy with respect to $\hj$) given by $||J^*-J_{\hu}||$ (Theorem~\ref{polthe}).
		\item Our analysis is based on two novel $\max$-norm contraction operators and our results hold \emph{with probability one}. This is another significant difference in comparison to the results on constraint sampling in \cite{SALP,CS} that make use of concentration bounds and hold only with \emph{high} probability.
		\item Our results on the GRLP are the first to theoretically justify linear function approximation of the constraints.
		\item We demonstrate via an example in controlled queues that the experiments conform to the theory developed.
\end{enumerate}

