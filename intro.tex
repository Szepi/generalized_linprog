%!TEX root =  autocontgrlp.tex
\section{Introduction}\label{intro}
Optimal sequential decision making problems occurring in science, engineering and economics can be cast in the framework of Markov Decision Processes (MDPs). A policy $u$, is a mapping from state space to action space and it specifies the action selection mechanism. The problem of \emph{control} requires coming up with good policies and for which one needs to \emph{predict} the behavior of individual policies. The problem of \emph{prediction} deals with computing the value-function $J_u$(a mapping from state space to reals\footnote{Also known as reward/cost-to-go function.}) which specifies the value of each state under a given policy $u$. \todoc{How is the MDP given? If it is given as a list of values, just reading the values may take too much time.
Or we assume sparsity? Or we assume that one can query the next state distribution at any state-action? These yield to different models. We should specify which one we work with (presumably the sparse is the easiest).
}Ideally, we are be interested in computing the optimal value function $J^*$ and the optimal policy $u^*$. The Bellman equation (BE) relates these quantities and in particular $u^*$ can be obtained as a `greedy/one-step look ahead' policy by substituting $J^*$ in the BE. Conventional solution methods for MDPs \cite{BertB} such as value iteration, policy iteration and linear programming can compute exact values of $J^*$ and $u^*$ by either directly or indirectly solving the Bellman equation. By computing $J^*$ and $u^*$, the conventional methods address both the prediction and control problems. However, in the case of MDPs with large number of states, computing $u^*$ and $J^*$ using the exact methods is infeasible. \todoc{Do we have a source to cite here? What does it mean to solve an MDP anyways? Find an optimal policy? Find a near-optimal policy? I think the story is that knowing the optimal value function helps, but I have not seen any claim that it is \emph{necessary} to compute it to find.}


\emph{Approximate dynamic programming} (ADP) \cite{dpchapter,powell} refers to a gamut of approximate solution methods to MDPs with large number of states. Value-function-only\footnote{Since this paper is concerned with results related to the approximate linear programming formulation we do not discuss other classes of ADP methods such as \emph{actor-only} and \emph{actor-critic} methods.} (or \emph{critic-only}) ADP methods compute an approximate value function and use it to obtain a one-step look ahead policy by making use of BE. In order to reduce the computational effort, the approximate value function is restricted to a chosen parameterized class of functions. The most widely used parameterization to approximate the value function is the \emph{linear function approximator} (LFA). Methods employing LFA approximate the value function as $\Phi r^*$, where $\Phi$ is a feature matrix whose columns are the basis functions and $r^*$ is a weight vector to be learnt. Computational advantage (in terms of the number of unknowns) is achieved by choosing fewer number of basis functions compared to the number of states. Once the set of basis function is chosen, the way to compute $r^*$ varies across the different ADP methods.\todoc{I don't understand why one should care about  approximating $J^*$ at all.}

A host of value-function-based ADP methods solve the \emph{projected Bellman equation} (PBE) which is obtained by including the least squares projection operator in the BE so as to project quantities onto the lower dimensional subspace. A limitation of using the least squares projection operator is that it reduces the prediction error $L_2$-norm and cannot ensure policy improvement which requires prediction error to be minimum in the $L_\infty$-norm. Due to this norm mismatch, policy improvement cannot be guaranteed and \emph{policy-chattering} (policy-oscillations) can occur in practice \cite{dpchapter}. As a result, the performance loss is difficult to control and existing result that control this loss are proven under restrictive conditions \cite{anszemu:mlj07,FaMuSz10}.

The \emph{approximate linear programming} (ALP) \cite{ALP,CS,SALP,ALP-Bor,gkp,fs,npalp} formulation also employs LFA, however, solves a linear program to compute $J^*\approx\tilde{J}=\Phi r^*$, and computes a one-step greedy policy $\tilde{u}$. Since ALP computes an approximation to $J^*$ and outputs only a single policy $\tilde{u}$, there is no issue of policy-chattering. The ALP formulation also provides theoretical performance guarantees, i.e., the prediction error $||J^*-\tilde{J}||$ and the control error\footnote{loss in performance due to the sub-optimal policy} $||J^*-J_{\tilde{u}}||$ can be bounded. This makes ALP an attractive method since it addresses both the prediction and control problems by providing the error bounds. However, an important shortcoming of ALP is that the number of constraints are of the order of the state space, making the vanilla version of ALP intractable.

One proposal in the literature to overcome this hurdle is to employ a procedure known as constraint sampling, wherein a subset of the original constraints of the ALP are sampled to formulate a \emph{reduced linear program} (RLP). The RLP has been shown to perform well in experiments \cite{ALP,CS,CST} in various domains such as Tetris and in network of queues. However, the theoretical results are available only for a specific RLP formulated under idealized conditions \cite{CS}. Thus, there is a gap in the theoretical understanding of RLP and constraint reduction/approximation. Our aim in this paper is to fill this gap by providing theoretical guarantees which will make the RLP a complete ADP method that addresses both the prediction and the control problems.

In particular, in this paper we develop a novel theoretical framework to study a generalized constraint reduction technique namely \emph{generalized reduced linear program} (GRLP) and present its error analysis. The salient aspects of our contribution are listed below:
\begin{enumerate}
\item \textbf{Framework:}

We generalize RLPs to define GRLPs. A GRLP has a small number of constraints which are obtained as positive linear combinations of the original constraints of the ALP. 
The GRLP serves as a framework to analyze error due to constraint reduction and linear constraint approximation.
\item \textbf{Error Analysis:}
	\begin{itemize}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$. 
		\item We show a bound of the form $||J^*-\hat{J}||\leq c_1+c_2$, 
		where the term $c_1$ is the error inherent to the ALP formulation itself, 
		while the term $c_2$ is the additional error introduced due to the constraint approximation.  
		\item We derive a greedy policy $\hu$ by making use of $\hj$ and bound the loss $||J^*-J_{\hu}||$. 
		\item Akin to the error analysis in \cite{ALP,CS,SALP} our bounds are also in terms of a modified $L_\infty$-norm.
		\item Our analysis is based on two novel $\max$-norm contraction operators called the least upper bound (LUB) projection operator and the approximate least upper bound projection operator (ALUB). This is another significant difference in comparison to the results on constraint sampling in \cite{SALP,CS} that make use of concentration bounds and hold only with \emph{high} probability.
\end{itemize}
\item \textbf{Significant Results:}
	\begin{itemize}
		\item We provide error bounds for both prediction and control errors, i.e., bound the terms $||J^*-\hj||$ and $||J^*-J_{\hu}||$ in a modified $L_\infty$-norm. Thus, the GRLP is a complete ADP method with performance guarantees for both the prediction as well as control problems. The modified $L_\infty$-norm allows us to choose the quality of approximation across various states. This makes our result an important addition to the theory of ALP \cite{ALP,CS,CST,SALP}.
		\item The structure of the error terms also reveals an important result that it is not always necessary to sample using the stationary distribution of the optimal policy. This throws additional light on constraint sampling by providing a better explanation for its empirical success even in those cases when sampling distributions other than the stationary distribution of the optimal policy was used. 
		\item Our results on the GRLP are the first to theoretically justify linear function approximation of the constraints. This means that constraint reduction is not only limited to sampling but also can be extended to include linear combinations of constraints.
	\end{itemize}
\item We also discuss the implication of our results in the context of reinforcement learning and also present a numerical example illustrating the theory developed. A short and preliminary version of this paper without the theoretical analysis is available in \cite{aaaipaper}.
\end{enumerate}

