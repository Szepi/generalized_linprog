%!TEX root =  autocontgrlp.tex
\section{Introduction}
Markov decision processes (MDPs) are a powerful mathematical framework to study optimal sequential decision making problems  arising in science and engineering. In an MDP, the configuration of the system is described by the state variables (or simply the state) and the system evolves in a stochastic manner which depends on the action made. The set of all states is denoted by $S$, the state space and the set of all actions is denoted by $A$, the action space.
%An instance of an MDP is a controlled queue setting, where there is a cost associated with the number of customers in the queue, and the aim is to control the service level depending on the number of %customers so as to achieve a minimum cumulative cost. In more general terms, given any MDP,
We are interested in computing what is known as the optimal policy $u^*$ (a map from $S$ to $A$). The so-called dynamic programming (DP) methods first compute what is known as the optimal \emph{value-function} ($J^*$), a vector whose dimension is the number of states, and use it to compute $u^*$. When the number of state is small conventional DP techniques, such as value-, or policy-iteration, or linear programming (LP) can be used to compute $J^*$ and $u^*$\cite{BertB}.\par
As the number of states is grows, it becomes increasingly difficult to compute the exact values of $J^*$ and $u^*$. A practical solution then is to compute an approximate value function $\tilde{J}$ instead of $J^*$. Approximate dynamic programming (ADP) methods combine an approximation architecture to represent $\tj$ and a conventional DP method to compute $\tj$. Eventually, ADP methods output a sub-optimal policy $\tu$ using the $\tj$ they compute. %Here, success depends on the quality of approximation, i.e., on the quantity $||J^*-\tilde{J}||$ for an appropriately chosen norm.\par
Linear function approximation (LFA), i.e., letting $\tilde{J}=\Phi r^*$ where $\Phi$ is a so-called feature matrix and $r^*$ is a weight vector to be computed, is the most widely used method of approximation. Here, dimensionality reduction is achieved by choosing $\Phi$ to have fewer columns in comparison to the number of states, holding the promise of being able to work with MDPs regardless of the number of states.\par
It is natural to expect that approximations lead to errors and it is important to quantify the errors. For a given ADP method, theoretical performance analysis analytically bounding the error terms $||J^*-\tilde{J}||$  and $\norm{J^*-J_{\tu}}$ which denote the error in approximating the value function, and performance loss due to following policy $\tu$ (here $J_{\tu}$ is the value of $\tu$) respectively. Further, in most cases the error terms reveal some structure that can offer insights and act as guide to the designer of the ADP method (for example the choice of $\Phi$). \par

%While many ADP methods use LFA, not all of them are successful. For instance, ADP methods that use linear least squares projection are known to exhibit `policy oscillations' \cite{dpchapter}, i.e., %output a repeating sequence of bad sub-optimal policies.
%Such an analysis is important to establish that the error is always bounded.
%Further, in most cases the error terms reveal some structure that can offer insights and act as guide to the designer of the ADP method (for example the choice of $\Phi$). \par

The \emph{approximate linear program} (ALP) \cite{ALP,CS,SALP,ALP-Bor} and its variants combine LFA and the LP approach. Theoretical performance analysis of the ALP can be found in \cite{ALP}.
%and a salient feature is that it does not suffer from issues such as `policy oscillations'\footnote{ADP methods that use linear least squares projection are known to exhibit `policy oscillations' %\cite{BertB}, i.e., output a repeating sequence of bad sub-optimal policies. Since our focus in this paper is the ALP formulation, we refrain from a detailed presentation of the other methods.}

A critical shortcoming of vanilla ALP is that the number of constraints are of the order of the size of the state space, making this vanilla version intractable. Two approaches have been found empircally successful in addressing the issue of large number of constraints in the ALP. In the first approach, a random subset of constraints is chosen (dropping the rest), thereby formulating a \emph{reduced linear program} (RLP). The performance analysis of the RLP can be found in \cite{CS}, however, the bounds hold only in high probability under idealized assumptions. The second approach involves employing function approximation in the dual variables of the ALP \cite{ALP-Bor,dolgov}. However, to this date, there exist no theoretical guarantees bounding the loss in performance due to this approach.\par
Our motivation stems from the fact that ALP with tractable number of constraints will result in a full dimeniosnality free ADP method. However, constraint reduction in the ALP is an extra source of error (in addition to the error due to LFA), which has not been theoretically well understood.  The focus of this paper is to fill this gap in theory by deriving performance bounds for constraint reduction in the ALP formulation.
The salient aspects of our contribution are listed below:
\begin{enumerate}
\item We define a generalized reduced linear program (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints. The GRLP amounts to linear function approximation of the dual variables, and the RLP is a special case of GRLP.
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$ by bounding the prediction error $||J^*-\hj||$ (\Cref{cmt2mn}). 
		\item We also bound the performance loss due to using the policy $\hu$ that is one-step greedy with respect to $\hj$ (Theorem~\ref{polthe}).
		\item Our analysis is based on two novel $\max$-norm contraction operators and our results hold \emph{deterministically}, as opposed to the results on RLP \cite{SALP,CS}, where the guarantees have a probabilistic nature.
%Our analysis also makes use of arguments based on \emph{Lyanpunov} function, an approach much similar to prior works in ALP literature \cite{ALP,SALP}.
\item Our results on the GRLP are the first to theoretically analyze the use of linear function approximation of Langrangian (dual) variables underlying the constraints.
\item A numerical example in controlled queues is provided to illustrate the theory.
\end{enumerate}
%ALP with tractable number of constraints would result in  a full dimensionality free ADP method, without issues such as policy oscillations.
A short and preliminary version of this paper without the theoretical analysis is available in \cite{aaaipaper}.
