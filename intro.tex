%!TEX root =  autocontgrlp.tex
\section{Introduction}
Markov decision processes (MDPs) are a powerful mathematical framework to study optimal sequential decision making problems  arising in science and engineering. The so-called dynamic programming methods find an optimal policy by computing the optimal \emph{value-function} ($J^*$), a vector whose dimension is the number of states. MDPs with small number of states can be solved easily by conventional dynamic programming techniques, such as value-, or policy-iteration, or linear programming (LP) \cite{BertB}. 

In this paper we consider the problem of using dynamic programming with MDPs with large state spaces.
A practical way to tackle the issue of large number of states is to compute an approximate value function $\tilde{J}$ instead of $J^*$. %Here, success depends on the quality of approximation, i.e., on the quantity $||J^*-\tilde{J}||$ for an appropriately chosen norm. 
Linear function approximation (LFA), i.e., letting $\tilde{J}=\Phi r^*$ where $\Phi$ is a so-called feature matrix and $r^*$ is a weight vector to be computed, is the most widely used method of approximation. Here, dimensionality reduction is achieved by choosing $\Phi$ to have fewer columns in comparison to the number of states, holding the promise of being able to work with MDPs regardless of the number of states.

The \emph{approximate linear program} (ALP) \cite{ALP,CS,SALP,ALP-Bor} and its variants introduce linear function approximation in the linear programming formulation. A critical shortcoming of vanilla ALP is that the number of constraints are of the order of the size of the state space, making this vanilla version intractable. A way out is to choose a subset of constraints at random and drop the rest, thereby formulating a \emph{reduced linear program} (RLP). The performance analysis of the RLP can be found in \cite{CS} and the RLP has also been shown to perform well in experiments \cite{ALP,CS,CST}. An alternative approach to handle the issue of large number of constraints is to employ function approximation in the dual variables of the ALP \cite{ALP-Bor,dolgov}, an approach that was also found useful in experiments. However, to this date, there exist no theoretical guarantees bounding the loss in performance resulting from such an approximation.

In this paper, we generalize the RLP to define a generalized reduced linear program (GRLP) which has a tractable number of constraints that are obtained as positive linear combinations of the original constraints.
The salient aspects of our contribution are listed below:
\begin{enumerate}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$ by bounding the prediction error $||J^*-\hj||$ (\Cref{cmt2mn}). 
		\item We also bound the performance loss due to using the policy $\hu$ that is one-step greedy with respect to $\hj$ (Theorem~\ref{polthe}).
		\item Our analysis is based on two novel $\max$-norm contraction operators and our results hold \emph{deterministically}, as opposed to the results on RLP \cite{SALP,CS}, where the guarantees have a probabilistic nature.
		\item Our results on the GRLP are the first to theoretically analyze the use of linear function approximation of Langrangian (dual) variables underlying the constraints.
		\item A numerical example in controlled queues is provided to illustrate the theory.
\end{enumerate}

