%!TEX root =  autocontgrlp.tex
\section{Introduction}\label{intro}
Optimal sequential decision making problems occurring in science, engineering and economics can be cast in the framework of Markov Decision Processes (MDPs). A policy $u$, is a mapping from state space to action space and it specifies the action selection mechanism. The problem of \emph{control} requires coming up with good policies and for which one needs to \emph{predict} the behavior of individual policies. The problem of \emph{prediction} deals with computing the value-function $J_u$\footnote{Also known as reward/cost-to-go function.} (a mapping from state space to reals) which specifies the value of each state under a given policy $u$. In order to solve an MDP it is necessary to solve the problem of prediction as well as the problem of control.\\
Given an MDP, ideally, we would be interested in computing the optimal value function $J^*$ and the optimal policy $u^*$. The Bellman equation (BE) relates these quantities. Conventional solution methods for MDPs \cite{BertB} such as value iteration, policy iteration and linear programming can compute exact values of $J^*$ and $u^*$ by either directly or indirectly solving the Bellman equation. By computing $J^*$ and $u^*$, the conventional methods address both the prediction and control problems. However, in most practical problems the size of the state space grows exponentially in the number of state variables, making the computation of $u^*$ and $J^*$ intractable using the exact methods.\\
Approximate Dynamic Programming (ADP) \cite{dpchapter,powell} refers to a gamut of approximate solution methods to MDPs with large number of states. In order to reduce the computational effort, ADP methods resort to dimensionality reduction by parameterizing the value function and/or the policy and restricting their search space to the chosen parameterized class. Thus, instead of computing exact values of $u^*$ and $J^*$, ADP methods only compute an approximate value function $\tilde{J}$ and a sub-optimal policy $\tilde{u}$. The quality of the ADP method is measured by the error $||J^*-\tilde{J}||$, in approximating the value function and the loss in performance due to the sub-optimal policy $||J^*-J_{\tilde{u}}||$, measured with respect to the optimal policy. Of the two error terms, $||J^*-J_{\tu}||$ is more important because ultimately we are interested in the problem of finding a useful policy. In the context of ADP, the prediction and control problems are said to be addressed when the errors $||J^*-\tilde{J}||$ and $||J^*-J_{\tilde{u}}||$ can be bounded so that the loss in performance due to the approximation can be quantified.\\
Value-function-only\footnote{Since this paper is concerned with results related to the Approximate Linear Programming formulation we do not discuss other classes of ADP methods such as \emph{actor-only} and \emph{actor-critic} methods.} (or \emph{critic-only}) ADP methods approximate the value function and use it to obtain a one-step look ahead policy by making use of the Bellman equation. Most widely used parameterization to approximate the value function is the linear function approximator (LFA). Methods employing LFA approximate the value function as $J_u\approx\Phi r^*$, where $\Phi$ is a feature matrix whose columns are the basis functions and $r^*$ is a weight vector to be learnt. Dimensionality reduction is achieved by choosing fewer number of basis functions compared to the number of states.\\
A host of value-function-based ADP methods solve the Projected Bellman equation (PBE) which is obtained by including the least squares projection operator in the BE so as to project quantities onto the lower dimensional subspace. A limitation of using the least squares projection operator is that it reduces the prediction error $L_2$-norm and cannot ensure policy improvement which requires prediction error to be minimum in the $L_\infty$-norm. Due to this norm mismatch, policy improvement cannot be guaranteed and \emph{policy-chattering} (policy-oscillations) can occur in practice \cite{dpchapter}. 
As a result, the performance loss is difficult to control and existing result that control this loss are proven under restrictive conditions \cite[e.g.,][]{anszemu:mlj07,FaMuSz10}.\\
The approximate linear programming (ALP) \cite{ALP,CS,SALP,ALP-Bor,gkp,fs,npalp} formulation also employs LFA, however, solves a linear program to compute $J^*\approx\tilde{J}=\Phi r^*$, and computes a one-step greedy policy. Since ALP computes an approximation to $J^*$ and outputs only a single policy $\tilde{u}$, there is no issue of policy-chattering. The ALP formulation also provides theoretical performance guarantees, i.e., both error terms $||J^*-\tilde{J}||$ and $||J^*-J_{\tilde{u}}||$ can be bounded. This makes ALP an attractive method since it addresses both the prediction and control problems. However, an important shortcoming of ALP is that the number of constraints are of the order of the state space, making the vanilla version of ALP intractable.\\
One proposal in the literature to overcome this hurdle is to employ a procedure known as constraint sampling, wherein a subset of the original constraints of the ALP are sampled to formulate a reduced linear program (RLP). The RLP has been shown to perform well in experiments \cite{ALP,CS,CST} in various domains such as Tetris and in network of queues. However, the theoretical results \cite{CS} are available only for a specific RLP formulated under idealized conditions. Thus, there is a gap in the theoretical understanding of RLP and constraint reduction/approximation. Our aim in this paper is to provide theoretical guarantees which will make the RLP a complete ADP method that addresses both the prediction and the control problems.\\
In this paper, we develop a novel theoretical framework to study a generalized constraint reduction technique namely generalized reduced linear program (GRLP) and present its error analysis. The salient aspects of our contribution are listed below:
\begin{enumerate}
\item \textbf{Framework:}\\ We generalize the RLP to define GRLPs. A GRLP has a small number of constraints which are obtained as positive linear combinations of the original constraints of the ALP. The GRLP serves as a framework to analyze error due to constraint reduction and linear constraint approximation.
\item \textbf{Error Analysis:}
	\begin{itemize}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$. 
		\item We show that $||J^*-\hat{J}||\leq (c_1+c_2)$, where $c_1>0$, $c_2>0$ are constants. While the term $c_1$ is error inherent to the ALP formulation itself, the term $c_2$ is the additional error introduced due to constraint approximation.  
		\item We derive a greedy policy $\hu$ by making use of $\hj$ and bound the term $||J^*-J_{\hu}||$. 
		\item Akin to the error analysis in \cite{ALP,CS,SALP} our bounds are also in terms of a modified $L_\infty$-norm.
		\item Our analysis is based on two novel $\max$-norm contraction operators called the least upper bound (LUB) projection operator and the approximate least upper bound projection operator (ALUB). This is another significant difference in comparison to the results on constraint sampling in \cite{SALP,CS} that make use of concentration bounds and hold only with \emph{high} probability.
\end{itemize}
\item \textbf{Significant Results:}
	\begin{itemize}
		\item We provide error bounds for both prediction and control errors, i.e., bound the terms $||J^*-\hj||$ and $||J^*-J_{\hu}||$ in a modified $L_\infty$-norm. Thus, the GRLP is a complete ADP method with performance guarantees for both the prediction as well as control problems. The modified $L_\infty$-norm allows us to choose the quality of approximation across various states. This makes our result an important addition to the theory of ALP \cite{ALP,CS,CST,SALP}.
		\item The structure of the error terms also reveals an important result that it is not always necessary to sample using the stationary distribution of the optimal policy. This throws additional light on constraint sampling by providing a better explanation for its empirical success even in those cases when sampling distributions other than the stationary distribution of the optimal policy was used. 
		\item Our results on the GRLP are the first to theoretically justify linear function approximation of the constraints. This means that constraint reduction is not only limited to sampling but also can be extended to include linear combinations of constraints.
	\end{itemize}
\item We also discuss the implication of our results in the context of reinforcement learning and also present a numerical example illustrating the theory developed. A short and preliminary version of this paper without the theoretical analysis is available in \cite{aaaipaper}.
\end{enumerate}
