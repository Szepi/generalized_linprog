%!TEX root =  autocontgrlp.tex
\section{Introduction}\label{intro}
Optimal sequential decision making problems occurring in science, engineering and economics can be cast in the framework of Markov Decision Processes (MDPs), where the problem is to find a policy $u$, mapping states to actions, so as to maximize long term expected cumulated discounted reward. 
The problem of \emph{control} requires coming up with good policies.
% and for which one needs to \emph{predict} the behavior of individual policies. 
	\todoc{I removed the second part of the sentence; it seemed that it was not adding anything.}
The problem of \emph{prediction} deals with computing the value-function $J_u$ 
	(a mapping from the state space to the set of reals%
	\footnote{Also known as reward/cost-to-go function.}), 
	which gives the value of each state under a given policy $u$. 
\todoc{How is the MDP given? If it is given as a list of values, just reading the values may take too much time.
Or we assume sparsity? Or we assume that one can query the next state distribution at any state-action? These yield to different models. We should specify which one we work with (presumably the sparse is the easiest).
}
\todoc{Why do we discuss control and prediction?}
The optimal value function $J^*$ collects the highest values achievable in each state.
A policy $u^*$ is optimal if it achieves the optimal value in each state, i.e., if $J^* = J_{u^*}$.
A key observation that goes back to Bellman is that $J^*$ is the solution of a nonlinear fixed point equation,
	known as the Bellman equation (BE),
	and that an optimal policy can be obtained via the knowledge of $J^*$ through this fixed point equation.
Dynamic programming based solution methods for MDPs \cite{BertB},
	such as value iteration, policy iteration and linear programming,
	hence compute $J^*$ to then obtain an optimal policy.
%Ideally, we would be interested in computing the optimal value function $J^*$ and the optimal policy $u^*$. 
%Under reasonable conditions, the optimal value function is known to be the fixed point of the (so-called) Bellman equation (BE),
%in which case $u^*$ can be obtained as a `greedy/one-step look ahead' policy by substituting $J^*$ in the BE. 
%``Exact'' solution methods for MDPs \cite{BertB} such as value iteration, policy iteration and linear programming 
%compute $J^*$ and $u^*$ by either directly or indirectly solving the Bellman equation. 
By computing $J^*$ and $u^*$, the conventional methods address both the prediction and control problems. 
\todoc{So is the prediction problem to predict $J_{u^*}$?}
However, in the case of MDPs with large number of states, computing $u^*$ and $J^*$ using 
	these exact methods is infeasible. 


\emph{Approximate dynamic programming} (ADP) \cite{dpchapter,powell} 
	refers to a gamut of approximate solution methods to MDPs with large number of states
	that build on dynamic programming.
The key idea in value-function-only%
\footnote{
		Since this paper is concerned with results related to the approximate linear programming formulation we do not 
		discuss other classes of ADP methods such as \emph{actor-only} and \emph{actor-critic} methods.} 
	(or \emph{critic-only}) ADP methods is to employ a parametrized class of functions
	to approximate the optimal value function.
Linear parameterization is the most common.
Methods employing such a \emph{linear function approximator} (LFA) 
	approximate the value function as $\Phi r^*$, 
	where $\Phi$ is a feature matrix whose columns are the basis functions 
	and $r^*$ is a weight vector to be learned. 
A representational advantage (in terms of the number of unknowns) 
	is achieved by choosing fewer number of basis functions compared to the number of states. 
Once the set of basis function is chosen, 
	the way to compute $r^*$ varies across the different ADP methods.

Building on policy iteration, a host of value-function-based ADP methods solve 
	the \emph{projected Bellman equation} (PBE) 
	which is obtained by including the least squares projection operator in the BE 
	so as to project the value function onto the lower dimensional subspace. 
A limitation of using the least squares projection operator is that 
	it minimizes the $L_2$-norm distance to the chosen subspace of linearly parameterized functions 
	and cannot in general ensure monotonous policy improvement 
	which requires the distance to be minimum in the $L_\infty$-norm. 
Due to this norm mismatch, in the lack of monotonous policy improvement 
	\emph{policy-chattering} (policy-oscillations) can occur \cite{dpchapter}. 
The performance loss can still be controlled \cite{FaMuSz10}, 
	though only at the price of posing extra, limiting assumptions on the MDP. 
	\todoc{How about value-iteration based methods? The paragraph
	should/could capture both of these..}
	
The \emph{approximate linear programming} (ALP) \cite{ALP,CS,SALP,ALP-Bor,gkp,fs,npalp} host of methods 
	also employs LFA. 
However, ALP solves a linear program to compute $J^*\approx\tilde{J}=\Phi r^*$, 
	based on which a one-step greedy policy $\tilde{u}$ is then computed as an approximation to $u^*$. 
Since ALP computes an approximation to $J^*$ and outputs only a single policy $\tilde{u}$, 
	there is no issue of policy-chattering. 
The ALP formulation also provides theoretical performance guarantees, 
	i.e., the prediction error $||J^*-\tilde{J}||$ and the control error%
	\footnote{That is, the loss in performance due to the sub-optimal policy} 
	$||J^*-J_{\tilde{u}}||$ can be bounded. 
This makes ALP an attractive method since it addresses both the prediction 
	and control problems by providing the error bounds. 
However, a critical shortcoming of ALP is 
	that the number of constraints are of the order of the state space, 
	making, in the lack of extra structure, 
	\todoc{I added this as sometimes one can solve linear programs with exponentially many constraints in polynomial time.}
	the vanilla version of ALP intractable.

One proposal in the literature to overcome this hurdle is 
	to employ a procedure known as constraint sampling, 
	wherein a subset of the original constraints of the ALP 
	are sampled to formulate a \emph{reduced linear program} (RLP). 
The RLP has been shown to perform well in experiments \cite{ALP,CS,CST} 
	in various domains such as Tetris and in network of queues. 
However, theoretical results are available only for a specific RLP 
	formulated under idealized conditions \cite{CS}. 
Thus, there is a gap in the theoretical understanding of RLP 
	and constraint reduction/approximation. 
Our aim in this paper is to fill this gap by providing theoretical guarantees 
	which will make the RLP a complete ADP method that addresses 
	both the prediction and the control problems. 

In particular, in this paper we develop a novel theoretical framework 
	to study a generalized constraint reduction technique 
	namely \emph{generalized reduced linear program} (GRLP) and present its error analysis. 
	The salient aspects of our contribution are listed below:
%
\begin{enumerate}
\item \textbf{Framework:}

We generalize RLPs to define GRLPs. A GRLP has a small number of constraints which are obtained as positive linear combinations of the original constraints of the ALP. 
The GRLP serves as a framework to analyze error due to constraint reduction and linear constraint approximation.
\todoc{Give theorem numbers?}
\item \textbf{Error Analysis:}
	\begin{itemize}
		\item We develop novel analytical machinery to relate $\hat{J}$, the solution to the GRLP, and the optimal value function $J^*$. 
		\item We show a bound of the form $||J^*-\hat{J}||\leq c_1+c_2$, 
		where the term $c_1$ is the error inherent to the ALP formulation itself, 
		while the term $c_2$ is the additional error introduced due to the constraint approximation.  
		\item We also bound the loss $||J^*-J_{\hu}||$ of the one-step greedy policy $\hu$ based on $\hj$.
		\item Akin to the error analysis in \cite{ALP,CS,SALP} our bounds are also in terms of a weighted $L_\infty$-norm.
		\item Our analysis is based on two novel $\max$-norm contraction operators called the least upper bound (LUB) projection operator and the approximate least upper bound projection operator (ALUB). This is another significant difference in comparison to the results on constraint sampling in \cite{SALP,CS} that make use of concentration bounds and hold only with \emph{high} probability.
\end{itemize}
\item \textbf{Significant Results:}
	\begin{itemize}
		\item We provide error bounds for both prediction and control errors, i.e., bound the terms $||J^*-\hj||$ and $||J^*-J_{\hu}||$ in a weighted $L_\infty$-norm. Thus, the GRLP is a complete ADP method with performance guarantees for both the prediction as well as control problems. The weighted $L_\infty$-norm allows us to choose the quality of approximation across various states. This makes our result an important addition to the theory of ALP \cite{ALP,CS,CST,SALP}.
		\item The structure of the error terms also reveals that it is not always necessary to sample using the stationary distribution of the optimal policy. This throws additional light on constraint sampling by providing a better explanation for its empirical success in the more practical situations when sampling distributions other than the stationary distribution of the optimal policy was used. 
		\item Our results on the GRLP are the first to theoretically justify linear function approximation of the constraints. This means that constraint reduction is not only limited to sampling but also can be extended to include linear combinations of constraints.
	\end{itemize}
\item We also discuss the implication of our results in the context of reinforcement learning and also present a numerical example illustrating the theory developed. A short and preliminary version of this paper without the theoretical analysis is available in \cite{aaaipaper}.
\end{enumerate}

