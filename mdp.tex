%!TEX root =  autocontgrlp.tex
\section{Markov Decision Processes (MDPs)}
We consider MDPs with large but finite number of states. Thus $S=\{1,2,\ldots,n\}$ for some large $n$ is the state space, and the action set is given by $A=\{1,2,\ldots,d\}$. For simplicity, we assume that all actions are feasible in all states. The probability transition kernel $P$ specifies the probability $p_a(s,s')$ of transitioning from state $s$ to state $s'$ under the action $a$. We denote the reward obtained for performing action $a\in A$ in state $s\in S$ by $g_a(s)$.\par
A stationary deterministic policy\footnote{For the scope of this paper, it suffices to restrict our attention to stationary deterministic policies.}(SDP) or simply a policy is a map $u\colon S\ra A$ which specifies the action selection mechanism. Given an SDP $u$, the infinite horizon discounted reward corresponding to state $s$ under $u$ is denoted by $J_u(s)$ and is defined by
\begin{align}
J_u(s)\stackrel{\Delta}{=}\E[\sum_{n=0}^\infty \alpha^n g_{a_n}(s_n)|s_0=s,a_n=u(s_n)\mbox{ }\forall n\geq 0],\nn
\end{align}
where $\alpha \in (0,1)$ is a given discount factor. Here $J_u(s)$ is known as the value of the state $s$ under SDP $u$, and the vector quantity $J_u\eqdef(J_u(s), \forall s\in S)\in R^n$ is called the value function corresponding to the SDP $u$. The \emph{optimal policy} $u^*$ is obtained as $u^*(s)\eqdef\arg\max_{u\in U}J_u(s)$\footnote{An optimal (SDP) $u^*$ exists and is well defined in the case of infinite horizon discounted reward MDP, for more details see \cite{BertB}.}, where $U$ is the class of all SDPs. The \emph{optimal value function} $J^*$ is the one obtained under the optimal policy, i.e., $J^*=J_{u^*}$.\par
Any optimal policy $u^*$ and value function $J^*$ obey the Bellman equation (BE): for all $ s \in S$, 
\begin{subequations}\label{bell}
\begin{align}
\label{bellval}J^*(s)&=\max_{ a\in A}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J^*(s')\big),\\
\label{bellpol}u^*(s)&= \underset{a\in A}{\argmax}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J^*(s')\big)\,.
\end{align}
\end{subequations}
Once $J^*$ is computed, $u^*$ can be obtained via \eqref{bellpol}. Thus computing the value function is at the heart of most solution methods to MDP.\par
For convenience, in what follows we allow any vector $J\in \R^n$ to be viewed as a real-valued function over $S$ via $J(s) = J_s$ and vice versa.
We now define some operators acting on $\R^n$ (equivalently on $\R^S$) that will be useful later.
%The Bellman operator $T$ is defined using the model parameters of the MDP as follows:
\begin{definition}
Let $c,\rho,\chi:S \to \R_+$ be positive valued functions, where $\R_+$ denotes the set of strictly positive reals. Then for $J\in \R^n$, $a\in A$ and $s\in S$, 
define
\begin{enumerate}[(i)]
\item The Bellman operator $T\colon \R^n \ra \R^n$ as $(TJ)(s)=\max_{a \in A}\big(g_a(s)+\alpha \sum_{s'} p_a(s,s')J(s')\big).
$
\item \label{bellactval} The Bellman operator (of action values) $H: \R^n \to \R^{nd}$ for state-action values as $HJ=[ H_1 J,\cdots,H_d J]^\top\in \R^{nd},$ where $(H_a J)(s)= g_a(s)+\alpha \sum_{s'}p_a(s,s') J(s')$.
\item The weighted $L_1$-norms $\norm{\cdot}_{1,c}$ 
and 
the weighted $L_\infty$-norms  $\norm{\cdot}_{\infty,\rho}$ as $
||J||_{1,c}=\sum_{s \in S} c(s)|J(s)|, 
||J||_{\infty,\rho}=\max_{s \in S} \frac{|J(s)|}{\rho(s)}$.
\item The discounted maximal inflation of $\chi$ due to $P = (p_a)_{a\in A}$ as $\beta_{\chi}=\max_{s \in S} \frac{\underset{a \in A}{\max}\big(\alpha\sum_{s'}p_a(s,s')\chi(s')\big)}{\chi(s)}$.
\item The function $\chi:S\to\R_+$ above to be a \emph{Lyapunov} function for $P = (p_a)_{a\in A}$ if $\beta_{\chi}<1$.
\item $E$ to be the $nd\times n$ matrix given by $E=[I,\ldots,I]^\top$, i.e., $E$ is obtained by stacking $d$ identical $n\times n$ identity matrices one over the other.
\item The greedy policy with respect to the value function $\tj$ as \begin{align*}\tu\eqdef\underset{a\in A}{\argmax}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')\tj(s')\big).\end{align*}
\end{enumerate}
\end{definition}
We now state without proof the most important properties of the Bellman operator.
The proofs are immediate from the definitions, but can also be found in \cite{BertB}.
First, we introduce some extra notation:
For $J_1,J_2\in \R^n$, we write $J_1\le J_2$ if $J_1(s)\le J_2(s)$ holds for all $s\in S$.
We use $\one \in \R^n$ to denote a vector with all entries $1$.
The maximum norm $\norm{\cdot}_{\infty}$ is defined by $ \norm{v}_{\infty} = \max_{s\in S} |v(s)|$.
\begin{lemma}\label{monotone}
$T$ is a monotone map, i.e., given $J_1,J_2 \in \R^n$ such that $J_1\leq J_2$, we have $T J_1\leq T J_2$. 
\end{lemma}
\begin{lemma}\label{shift}
Given $J\in \R^n$ and $t \in \R$, we have
\begin{align}\label{eq:shift}
T(J+t\one)=TJ+\alpha t\one.
\end{align}
\end{lemma}
\begin{lemma}\label{maxnorm}
If $T: \R^n \to \R^n$ is any operator that is monotonous and satisfies~\eqref{eq:shift} then 
$T$ is a $\max$-norm contraction operator with contraction factor $\alpha \in (0,1)$, i.e., given $J_1, J_2 \in \mathbf{R}^n$,
\begin{align}
||TJ_1-TJ_2||_\infty\leq \alpha ||J_1-J_2||_\infty.
\end{align}
\end{lemma}
\begin{lemma}\label{uniquesol} 
$J^*$ is a unique fixed point of $T$, i.e., $J^*=TJ^*$.
\end{lemma}
\begin{corollary}
If $J\in \R^n$ is such that $J\geq TJ$ then $J\geq TJ^2\geq \ldots \geq J^*$.
\end{corollary}
Though in \cref{monotone,shift,maxnorm}, we make use of the Bellman operator $T$, the results also hold for $H$ as well.
\subsection{Linear Programming Formulation}
The optimal value function $J^*$ can be obtained by solving the following linear program:
\begin{align}\label{mdplp}
\min_{J\in \R^n}\, &c^\top J\\
\text{s.t.}\mb &J(s)\geq g_a(s)+\alpha\sum_{s'}p_a(s,s')J(s'),\nn\\
&\forall s \in S, a \in A,
\end{align}
where $c$ is a probability distribution such that $c(s)>0, \forall s \in S$ and $\sum_{s\in S}c(s)=1$.\par
When the MDP has a large number of states, it is difficult to solve for $J^*$ using either the linear program \eqref{mdplp} or other full state representation methods such as value iteration or policy iteration \cite{BertB}. A practical solution is to resort to function approximation. The most widely used is the linear function approximation wherein the exact value function is approximated by a function that is obtained as a linear combination of a set of pre-selected basis functions.
