%!TEX root =  autocontgrlp.tex
\section{Markov Decision Processes (MDPs)}
We consider MDPs with a finite state space $S=\{1,2,\ldots,n\}$ and finite action space $A=\{1,2,\ldots,d\}$, where the sentiment is that $n$ is large. For simplicity, we assume that all actions are feasible in all states. The probability transition kernel $P$ collects the probabilities $p_a(s,s')$ of transitioning from state $s$ to state $s'$ under the action $a$ for all possible $s,s'\in S$ and $a\in A$. We denote the reward (or gain) obtained for performing action $a\in A$ in state $s\in S$ by $g_a(s)$.

A stationary deterministic policy\footnote{For the scope of this paper, it suffices to restrict our attention to stationary deterministic policies.} (SDP), or simply a policy, is a map $u\colon S\ra A$ that specifies for each state what action to select in that state.
Given an SDP $u$, the expected total discounted reward corresponding to starting at state $s$, while choosing actions as dictated by $u$ for the states encountered, is
\begin{align}
J_u(s)\stackrel{\Delta}{=}\E[\sum_{n=0}^\infty \alpha^n g_{a_n}(s_n)|s_0=s,a_n=u(s_n)\mbox{ }\forall n\geq 0],\nn
\end{align}
where $\alpha \in (0,1)$ is the so-called discount factor and $s_{n+1} \sim p_{a_n}(s_n,\cdot)$, $n\ge 0$. 
We call $J_u(s)$ the value of state $s$ under SDP $u$, while $J_u$ (as a map from $S$ to the reals)
is called the value function underlying policy $u$.
The \emph{optimal policy} $u^*$ is one that in each state $s\in S$ achieves the best possible total expected discounted reward from that state. That is, $J^{u^*}(s) = J^*(s) \eqdef \max_{u\in U} J_u(s)$
where $U$ is the set of all SDPs and $J^*$ is coined the \emph{optimal value function}.%
\footnote{In our case an optimal (SDP) $u^*$ exists and is well defined \cite{BertB}.}, 


Any optimal policy $u^*$ and value function $J^*$ obey the Bellman equation (BE): for all $ s \in S$, 
\begin{subequations}\label{bell}
\begin{align}
\label{bellval}J^*(s)&=\max_{ a\in A}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J^*(s')\big),\\
\label{bellpol}u^*(s)&= \underset{a\in A}{\argmax}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J^*(s')\big)\,.
\end{align}
\end{subequations}
Once $J^*$ is computed, $u^*$ can be obtained via \eqref{bellpol}
for each state relatively cheaply (e.g., having the ability to sample from the next-state distribution at
a given state-action pair).
Thus computing the value function is at the heart of most solution methods to MDP.

The value functions $J^u$ or $J^*$ are elements of $\R^S$.
In what follows it will be useful for us to treat these as $n$-dimensional vectors, i.e., elements of $\R^n$, effectively identifying $\R^S$ with $\R^S$ in the natural way. Similarly we identify $\R^{nd}$ with $\R^{S\times A}$.
The following definitions will be useful later:
%The Bellman operator $T$ is defined using the model parameters of the MDP as follows:
\begin{definition}
Let $c,\rho,\chi:S \to \R_+$ be positive valued functions, where $\R_+$ denotes the set of strictly positive reals. Then for $J\in \R^n$, $a\in A$ and $s\in S$, 
define
\begin{enumerate}[(i)]
\item The Bellman operator $T\colon \R^n \ra \R^n$ as $(TJ)(s)=\max_{a \in A}\big(g_a(s)+\alpha \sum_{s'} p_a(s,s')J(s')\big).
$
\item \label{bellactval} The Bellman operator (of action values) $H: \R^n \to \R^{nd}$ for state-action values as $HJ=[ H_1 J,\cdots,H_d J]^\top\in \R^{nd},$ where $(H_a J)(s)= g_a(s)+\alpha \sum_{s'}p_a(s,s') J(s')$.
\item The weighted $L_1$-norms $\norm{\cdot}_{1,c}$ 
and 
the weighted $L_\infty$-norms  $\norm{\cdot}_{\infty,\rho}$ as $
||J||_{1,c}=\sum_{s \in S} c(s)|J(s)|, 
||J||_{\infty,\rho}=\max_{s \in S} \frac{|J(s)|}{\rho(s)}$.
\item The discounted maximal inflation of $\chi$ due to $P = (p_a)_{a\in A}$ as $\beta_{\chi}=\max_{s \in S} \frac{\underset{a \in A}{\max}\big(\alpha\sum_{s'}p_a(s,s')\chi(s')\big)}{\chi(s)}$.
\item The function $\chi:S\to\R_+$ above to be a \emph{Lyapunov} function for $P = (p_a)_{a\in A}$ if $\beta_{\chi}<1$.
\item $E$ to be the $nd\times n$ matrix given by $E=[I,\ldots,I]^\top$, i.e., $E$ is obtained by stacking $d$ identical $n\times n$ identity matrices one over the other.
\item A policy $\tu$ is said to be greedy with respect to (w.r.t.) $\tj\in \R^n$ if for any $s\in S$,
%some function $\tj:S \to \R$ 
\begin{align*}\tu(s)\eqdef\underset{a\in A}{\argmax}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')\tj(s')\big).\end{align*}
\end{enumerate}
\end{definition}
We now state without proof the most important properties of the Bellman operator(s).
The proofs are immediate from the definitions, but can also be found in \cite{BertB}.
First, we introduce some extra notation:
For $J_1,J_2\in \R^n$, we write $J_1\le J_2$ if $J_1(s)\le J_2(s)$ holds for all $s\in S$.
We use $\one \in \R^n$ to denote a vector with all entries $1$.
The maximum norm $\norm{\cdot}_{\infty}$ is defined by $ \norm{v}_{\infty} = \max_{s\in S} |v(s)|$.
\begin{lemma}\label{monotone}
$T$ is a monotone map, i.e., given $J_1,J_2 \in \R^n$ such that $J_1\leq J_2$, we have $T J_1\leq T J_2$. 
\end{lemma}
\begin{lemma}\label{shift}
Given $J\in \R^n$ and $t \in \R$, we have
\begin{align}\label{eq:shift}
T(J+t\one)=TJ+\alpha t\one.
\end{align}
\end{lemma}
\begin{lemma}\label{maxnorm}
If $T: \R^n \to \R^n$ is any operator that is monotonous and satisfies~\eqref{eq:shift} then 
$T$ is a $\max$-norm contraction operator with contraction factor $\alpha \in (0,1)$, i.e., given $J_1, J_2 \in \R^n$,
\begin{align}
||TJ_1-TJ_2||_\infty\leq \alpha ||J_1-J_2||_\infty.
\end{align}
\end{lemma}
\begin{lemma}\label{uniquesol} 
$J^*$ is a unique fixed point of $T$, i.e., $J^*=TJ^*$.
\end{lemma}
\begin{corollary}
If $J\in \R^n$ is such that $J\geq TJ$ then $J\geq TJ^2\geq \ldots \geq J^*$.
\end{corollary}
Though  \cref{monotone,shift,maxnorm} are stated for the Bellman operator $T$, the results also hold for $H$ as well.

\subsection{The Linear Programming Formulation}
As it is well known, the optimal value function $J^*$ can be obtained by solving the following linear program:
\begin{align}\label{mdplp}
\min_{J\in \R^n}\, &c^\top J\,\,\text{s.t.}\\
%\mb
 J(s)&\geq g_a(s)+\alpha\sum_{s'}p_a(s,s')J(s'),\,\,
\forall (s,a)\in S\times A,
\end{align}
where $c$ is \emph{any} vector of $\R^n$ such that $c(s)>0, \forall s \in S$. Without loss of generality, we may and will assume in what follows that $\sum_{s\in S}c(s)=1$, i.e., $c$ is a probability distribution over $S$.

When the MDP has a large number of states, it is difficult to solve for $J^*$ using either the linear program \eqref{mdplp} or other full state representation methods such as value iteration or policy iteration \cite{BertB}. A practical solution is to resort to function approximation. Linear function approximation, wherein the solution is searched in the subspace spanned by a set of pre-selected basis functions, is an attractive choice.
