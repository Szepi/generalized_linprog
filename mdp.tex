%!TEX root =  autocontgrlp.tex
\section{Markov Decision Processes (MDPs) and Linear Programming}
We consider MDPs with a finite state space $S=\{1,2,\ldots,n\}$ and finite action space $A=\{1,2,\ldots,d\}$, where the sentiment is that $n$ is large. For simplicity, we assume that all actions are feasible in all states. The probability transition kernel $P$ collects the probabilities $p_a(s,s')$ of transitioning from state $s$ to state $s'$ under the action $a$ for all possible $s,s'\in S$ and $a\in A$. We denote the reward (or gain) obtained for performing action $a\in A$ in state $s\in S$ by $g_a(s)$.
A stationary deterministic policy\footnote{For the scope of this paper, it suffices to restrict our attention to stationary deterministic policies.} (SDP), or simply a policy, is a map $u\colon S\ra A$ that specifies for each state what action to select in that state.
Given an SDP $u$, the expected total discounted reward corresponding to starting at state $s$, while choosing actions as dictated by $u$ for the states encountered, is
\begin{align}
J_u(s)\stackrel{\Delta}{=}\E[\sum_{n=0}^\infty \alpha^n g_{a_n}(s_n)|s_0=s,a_n=u(s_n)\mbox{ }\forall n\geq 0],\nn
\end{align}
where $\alpha \in (0,1)$ is the so-called discount factor and $s_{n+1} \sim p_{a_n}(s_n,\cdot)$, $n\ge 0$. 
We call $J_u(s)$ the value of state $s$ under SDP $u$, while $J_u$ (as a map from $S$ to the reals)
is called the value function underlying policy $u$.
The \emph{optimal policy} $u^*$ is one that in each state $s\in S$ achieves the best possible total expected discounted reward from that state. That is, $J^{u^*}(s) = J^*(s) \eqdef \max_{u\in U} J_u(s)$
where $U$ is the set of all SDPs and $J^*$ is coined the \emph{optimal value function}.%
\footnote{In our case an optimal (SDP) $u^*$ exists and is well defined \cite{BertB}.}, 
Any optimal policy $u^*$ and value function $J^*$ obey the Bellman equation (BE): for all $ s \in S$,
\begin{subequations}\label{bell}
\begin{align}
\label{bellval}J^*(s)&=\max_{ a\in A}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J^*(s')\big),\\
\label{bellpol}u^*(s)&= \underset{a\in A}{\argmax}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J^*(s')\big)\,.
\end{align}
\end{subequations}
Once $J^*$ is computed, $u^*$ can be obtained via \eqref{bellpol}
for each state relatively cheaply (e.g., having the ability to sample from the next-state distribution at
a given state-action pair).
Thus computing the value function is at the heart of most solution methods to MDP.\par
%\subsection{Linear Programming}
As it is well known, the optimal value function $J^*$ can be obtained by solving the following linear program:
\begin{align}\label{mdplp}
\begin{split}
\min_{J\in \R^n}\, &c^\top J\,\,\text{s.t.}\\
%\mb
J(s)&\geq g_a(s)+\alpha\sum_{s'}p_a(s,s')J(s'),\,\,
\forall (s,a)\in S\times A,
\end{split}
\end{align}
where $c$ is \emph{any} vector of $\R^n$ such that $c(s)>0, \forall s \in S$. Without loss of generality, we may and will assume in what follows that $\sum_{s\in S}c(s)=1$, i.e., $c$ is a probability distribution over $S$.\par
%\subsection{Notation}
The value functions $J^u$ or $J^*$ are elements of $\R^S$.
In what follows it will be useful for us to treat these as $n$-dimensional vectors, i.e., elements of $\R^n$, effectively identifying $\R^S$ with $\R^S$ in the natural way. Similarly we identify $\R^{nd}$ with $\R^{S\times A}$.
The following definitions will be useful later:
%The Bellman operator $T$ is defined using the model parameters of the MDP as follows:
\begin{definition}Define
\begin{comment}
Let $c,\rho,\chi:S \to \R_+$ be positive valued functions, where $\R_+$ denotes the set of strictly positive reals. Then for $J\in \R^n$, $a\in A$ and $s\in S$,
define
\end{comment}
\begin{enumerate}[(i)]
\item The Bellman operator $T\colon \R^n \ra \R^n$ as $(TJ)(s)=\max_{a \in A}\big(g_a(s)+\alpha \sum_{s'} p_a(s,s')J(s')\big).
$
\item \label{bellactval} The Bellman operator (of action values) $H: \R^n \to \R^{nd}$ for state-action values as $HJ=[ H_1 J,\cdots,H_d J]^\top\in \R^{nd},$ where $(H_a J)(s)= g_a(s)+\alpha \sum_{s'}p_a(s,s') J(s')$.
\begin{comment}
\item
||J||_{\infty,\rho}=\max_{s \in S} \frac{|J(s)|}{\rho(s)}$.
\item The discounted maximal inflation of $\chi$ due to $P = (p_a)_{a\in A}$ as $\beta_{\chi}=\max_{s \in S} \frac{\underset{a \in A}{\max}\big(\alpha\sum_{s'}p_a(s,s')\chi(s')\big)}{\chi(s)}$.
\item The function $\chi:S\to\R_+$ above to be a \emph{Lyapunov} function for $P = (p_a)_{a\in A}$ if $\beta_{\chi}<1$.
\end{comment}
\item $E$ to be the $nd\times n$ matrix given by $E=[I,\ldots,I]^\top$, i.e., $E$ is obtained by stacking $d$ identical $n\times n$ identity matrices one over the other.
\item A policy $u_J$ is said to be greedy with respect to (w.r.t.) $J\in \R^n$ if for any $s\in S$,
%some function $\tj:S \to \R$
\begin{align*}u_J(s)\eqdef\underset{a\in A}{\argmax}\big(g_a(s)+\alpha \sum_{s'}p_a(s,s')J(s')\big).\end{align*}
\item The weighted $L_1$-norms $\norm{\cdot}_{1,c}$ with respect to a probability distribution $c$ as $
||J||_{1,c}=\sum_{s \in S} c(s)|J(s)|$.
\item The (un)weighted $L_\infty$-norms $\norm{\cdot}_{1,\infty}$
and $\norm{\cdot}_{\infty,\rho}$, $\parallel J\parallel_{\infty}=\max_{s\in S}|J(s)|$ and $||J||_{\infty,\rho}=\max_{s \in S} \frac{|J(s)|}{\rho(s)}$.
\item $J_1\leq J_2$, when $J_1(s)\leq J_2(s),~\forall s\in S$.
\item $\one\in \R^n$ to be the vector whose all coordinates are $1$.
\end{enumerate}
\end{definition}
